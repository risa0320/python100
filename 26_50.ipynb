{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "26_50.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhXYUig2tU-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofREkoMhskkh",
        "colab_type": "text"
      },
      "source": [
        "## 26. 強調マークアップの除去\n",
        "25の処理時に，テンプレートの値からMediaWikiの強調マークアップ（弱い強調，強調，強い強調のすべて）を除去してテキストに変換せよ（参考: マークアップ早見表）．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c78r7uEjhAEr",
        "colab_type": "code",
        "outputId": "2c776fc4-8410-4369-c582-111f3471a614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "import re\n",
        "\n",
        "def get_uk_data(wiki_data):\n",
        "    for i in wiki_data:\n",
        "        title = json.loads(i)[\"title\"]\n",
        "        if title == \"イギリス\":\n",
        "            return json.loads(i)[\"text\"]\n",
        "        \n",
        "def is_invalid_word(target_data):\n",
        "    if \" \" not in target_data or \"=\" not in target_data:\n",
        "        return False\n",
        "    elif \"基礎情報\" in target_data:\n",
        "        return False\n",
        "    return True\n",
        "    \n",
        "def to_dict(sentence):\n",
        "    d = {}\n",
        "    for data in sentence.split(\"\\n\"):\n",
        "        if data == \"}}\":\n",
        "            break\n",
        "        if not is_invalid_word(data):\n",
        "            continue\n",
        "        key = data.split(\"=\")[0].replace(\"|\", \"\").strip()\n",
        "        val = re.sub(\"'\", \"\", data.split(\"=\")[1]).strip()\n",
        "        d[key] = val\n",
        "    return d\n",
        "            \n",
        "with gzip.open(\"/content/drive/My Drive/jawiki-country.json.gz\", mode=\"rt\") as f:\n",
        "    wiki_data = f.readlines()\n",
        "uk_data = get_uk_data(wiki_data)\n",
        "dict_data = to_dict(uk_data)\n",
        "dict_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'GDP/人': '36,727<ref name',\n",
              " 'GDP値': '2兆3162億<ref name',\n",
              " 'GDP値MER': '2兆4337億<ref name',\n",
              " 'GDP値元': '1兆5478億<ref name',\n",
              " 'GDP統計年': '2012',\n",
              " 'GDP統計年MER': '2012',\n",
              " 'GDP統計年元': '2012',\n",
              " 'GDP順位': '6',\n",
              " 'GDP順位MER': '5',\n",
              " 'ISO 3166-1': 'GB / GBR',\n",
              " 'ccTLD': '[[.uk]] / [[.gb]]<ref>使用は.ukに比べ圧倒的少数。</ref>',\n",
              " '人口値': '63,181,775<ref>[http://esa.un.org/unpd/wpp/Excel-Data/population.htm United Nations Department of Economic and Social Affairs>Population Division>Data>Population>Total Population]</ref>',\n",
              " '人口大きさ': '1 E7',\n",
              " '人口密度値': '246',\n",
              " '人口統計年': '2011',\n",
              " '人口順位': '22',\n",
              " '位置画像': 'Location_UK_EU_Europe_001.svg',\n",
              " '元首等氏名': '[[エリザベス2世]]',\n",
              " '元首等肩書': '[[イギリスの君主|女王]]',\n",
              " '公式国名': '{{lang|en|United Kingdom of Great Britain and Northern Ireland}}<ref>英語以外での正式国名:<br/>',\n",
              " '公用語': '[[英語]]（事実上）',\n",
              " '国旗画像': 'Flag of the United Kingdom.svg',\n",
              " '国歌': '[[女王陛下万歳|神よ女王陛下を守り給え]]',\n",
              " '国章リンク': '（[[イギリスの国章|国章]]）',\n",
              " '国章画像': '[[ファイル:Royal Coat of Arms of the United Kingdom.svg|85px|イギリスの国章]]',\n",
              " '国際電話番号': '44',\n",
              " '夏時間': '+1',\n",
              " '建国形態': '建国',\n",
              " '日本語国名': 'グレートブリテン及び北アイルランド連合王国',\n",
              " '時間帯': '±0',\n",
              " '最大都市': 'ロンドン',\n",
              " '標語': '{{lang|fr|Dieu et mon droit}}<br/>（[[フランス語]]:神と私の権利）',\n",
              " '水面積率': '1.3%',\n",
              " '注記': '<references />',\n",
              " '略名': 'イギリス',\n",
              " '確立年月日1': '[[927年]]／[[843年]]',\n",
              " '確立年月日2': '[[1707年]]',\n",
              " '確立年月日3': '[[1801年]]',\n",
              " '確立年月日4': '[[1927年]]',\n",
              " '確立形態1': '[[イングランド王国]]／[[スコットランド王国]]<br />（両国とも[[連合法 (1707年)|1707年連合法]]まで）',\n",
              " '確立形態2': '[[グレートブリテン王国]]建国<br />（[[連合法 (1707年)|1707年連合法]]）',\n",
              " '確立形態3': '[[グレートブリテン及びアイルランド連合王国]]建国<br />（[[連合法 (1800年)|1800年連合法]]）',\n",
              " '確立形態4': '現在の国号「グレートブリテン及び北アイルランド連合王国」に変更',\n",
              " '通貨': '[[スターリング・ポンド|UKポンド]] (&pound;)',\n",
              " '通貨コード': 'GBP',\n",
              " '面積値': '244,820',\n",
              " '面積大きさ': '1 E11',\n",
              " '面積順位': '76',\n",
              " '首相等氏名': '[[デーヴィッド・キャメロン]]',\n",
              " '首相等肩書': '[[イギリスの首相|首相]]',\n",
              " '首都': '[[ロンドン]]'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tqtMg8fvhs_",
        "colab_type": "text"
      },
      "source": [
        "## 27. 内部リンクの除去\n",
        "26の処理に加えて，テンプレートの値からMediaWikiの内部リンクマークアップを除去し，テキストに変換せよ（参考: マークアップ早見表）．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HAMEgokviG2",
        "colab_type": "code",
        "outputId": "92aaf208-bcc5-4706-ee43-f281fb19e569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "import re\n",
        "\n",
        "def get_uk_data(wiki_data):\n",
        "    for i in wiki_data:\n",
        "        title = json.loads(i)[\"title\"]\n",
        "        if title == \"イギリス\":\n",
        "            return json.loads(i)[\"text\"]\n",
        "        \n",
        "def is_invalid_word(target_data):\n",
        "    if \" \" not in target_data or \"=\" not in target_data:\n",
        "        return False\n",
        "    elif \"基礎情報\" in target_data:\n",
        "        return False\n",
        "    return True\n",
        "    \n",
        "def to_dict(sentence):\n",
        "    d = {}\n",
        "    for data in sentence.split(\"\\n\"):\n",
        "        if data == \"}}\":\n",
        "            break\n",
        "        if not is_invalid_word(data):\n",
        "            continue\n",
        "        key = data.split(\"=\")[0].replace(\"|\", \"\").strip()\n",
        "        val = re.sub(\"'|\\[\\[|\\]\\]\", \"\", data.split(\"=\")[1]).split(\"<ref\")[0].strip()\n",
        "        d[key] = val\n",
        "    return d\n",
        "            \n",
        "with gzip.open(\"/content/drive/My Drive/jawiki-country.json.gz\", mode=\"rt\") as f:\n",
        "    wiki_data = f.readlines()\n",
        "uk_data = get_uk_data(wiki_data)\n",
        "dict_data = to_dict(uk_data)\n",
        "dict_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'GDP/人': '36,727',\n",
              " 'GDP値': '2兆3162億',\n",
              " 'GDP値MER': '2兆4337億',\n",
              " 'GDP値元': '1兆5478億',\n",
              " 'GDP統計年': '2012',\n",
              " 'GDP統計年MER': '2012',\n",
              " 'GDP統計年元': '2012',\n",
              " 'GDP順位': '6',\n",
              " 'GDP順位MER': '5',\n",
              " 'ISO 3166-1': 'GB / GBR',\n",
              " 'ccTLD': '.uk / .gb',\n",
              " '人口値': '63,181,775',\n",
              " '人口大きさ': '1 E7',\n",
              " '人口密度値': '246',\n",
              " '人口統計年': '2011',\n",
              " '人口順位': '22',\n",
              " '位置画像': 'Location_UK_EU_Europe_001.svg',\n",
              " '元首等氏名': 'エリザベス2世',\n",
              " '元首等肩書': 'イギリスの君主|女王',\n",
              " '公式国名': '{{lang|en|United Kingdom of Great Britain and Northern Ireland}}',\n",
              " '公用語': '英語（事実上）',\n",
              " '国旗画像': 'Flag of the United Kingdom.svg',\n",
              " '国歌': '女王陛下万歳|神よ女王陛下を守り給え',\n",
              " '国章リンク': '（イギリスの国章|国章）',\n",
              " '国章画像': 'ファイル:Royal Coat of Arms of the United Kingdom.svg|85px|イギリスの国章',\n",
              " '国際電話番号': '44',\n",
              " '夏時間': '+1',\n",
              " '建国形態': '建国',\n",
              " '日本語国名': 'グレートブリテン及び北アイルランド連合王国',\n",
              " '時間帯': '±0',\n",
              " '最大都市': 'ロンドン',\n",
              " '標語': '{{lang|fr|Dieu et mon droit}}<br/>（フランス語:神と私の権利）',\n",
              " '水面積率': '1.3%',\n",
              " '注記': '',\n",
              " '略名': 'イギリス',\n",
              " '確立年月日1': '927年／843年',\n",
              " '確立年月日2': '1707年',\n",
              " '確立年月日3': '1801年',\n",
              " '確立年月日4': '1927年',\n",
              " '確立形態1': 'イングランド王国／スコットランド王国<br />（両国とも連合法 (1707年)|1707年連合法まで）',\n",
              " '確立形態2': 'グレートブリテン王国建国<br />（連合法 (1707年)|1707年連合法）',\n",
              " '確立形態3': 'グレートブリテン及びアイルランド連合王国建国<br />（連合法 (1800年)|1800年連合法）',\n",
              " '確立形態4': '現在の国号「グレートブリテン及び北アイルランド連合王国」に変更',\n",
              " '通貨': 'スターリング・ポンド|UKポンド (&pound;)',\n",
              " '通貨コード': 'GBP',\n",
              " '面積値': '244,820',\n",
              " '面積大きさ': '1 E11',\n",
              " '面積順位': '76',\n",
              " '首相等氏名': 'デーヴィッド・キャメロン',\n",
              " '首相等肩書': 'イギリスの首相|首相',\n",
              " '首都': 'ロンドン'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_10pubYx97TX",
        "colab_type": "text"
      },
      "source": [
        "## 28. MediaWikiマークアップの除去\n",
        "27の処理に加えて，テンプレートの値からMediaWikiマークアップを可能な限り除去し，国の基本情報を整形せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ58rx0v97Ha",
        "colab_type": "code",
        "outputId": "b25e06df-a42f-42f9-c479-d9ceb089298b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "import gzip\n",
        "import json\n",
        "import re\n",
        "\n",
        "def get_uk_data(wiki_data):\n",
        "    for i in wiki_data:\n",
        "        title = json.loads(i)[\"title\"]\n",
        "        if title == \"イギリス\":\n",
        "            return json.loads(i)[\"text\"]\n",
        "        \n",
        "def is_invalid_word(target_data):\n",
        "    if \" \" not in target_data or \"=\" not in target_data:\n",
        "        return False\n",
        "    elif \"基礎情報\" in target_data:\n",
        "        return False\n",
        "    return True\n",
        "    \n",
        "def to_dict(sentence):\n",
        "    d = {}\n",
        "    for data in sentence.split(\"\\n\"):\n",
        "        if data == \"}}\":\n",
        "            break\n",
        "        if not is_invalid_word(data):\n",
        "            continue\n",
        "        key = data.split(\"=\")[0].replace(\"|\", \"\").strip()\n",
        "        val = re.sub(\"'|\\[\\[|\\]\\]\", \"\", data.split(\"=\")[1]).split(\"<ref\")[0].strip()\n",
        "        val = re.sub(\"<br.*>\", \"\", val)\n",
        "        if \"{{\" in val:\n",
        "            val = re.sub(\"\\{\\{|\\}\\}\", \"\", val).split(\"|\")[-1]\n",
        "        if \"ファイル:\" in val:\n",
        "            val = val.replace(\"ファイル:\", \"\")[1].split(\"|\")[0]\n",
        "        d[key] = val\n",
        "    return d\n",
        "            \n",
        "with gzip.open(\"/content/drive/My Drive/jawiki-country.json.gz\", mode=\"rt\") as f:\n",
        "    wiki_data = f.readlines()\n",
        "uk_data = get_uk_data(wiki_data)\n",
        "dict_data = to_dict(uk_data)\n",
        "dict_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'GDP/人': '36,727',\n",
              " 'GDP値': '2兆3162億',\n",
              " 'GDP値MER': '2兆4337億',\n",
              " 'GDP値元': '1兆5478億',\n",
              " 'GDP統計年': '2012',\n",
              " 'GDP統計年MER': '2012',\n",
              " 'GDP統計年元': '2012',\n",
              " 'GDP順位': '6',\n",
              " 'GDP順位MER': '5',\n",
              " 'ISO 3166-1': 'GB / GBR',\n",
              " 'ccTLD': '.uk / .gb',\n",
              " '人口値': '63,181,775',\n",
              " '人口大きさ': '1 E7',\n",
              " '人口密度値': '246',\n",
              " '人口統計年': '2011',\n",
              " '人口順位': '22',\n",
              " '位置画像': 'Location_UK_EU_Europe_001.svg',\n",
              " '元首等氏名': 'エリザベス2世',\n",
              " '元首等肩書': 'イギリスの君主|女王',\n",
              " '公式国名': 'United Kingdom of Great Britain and Northern Ireland',\n",
              " '公用語': '英語（事実上）',\n",
              " '国旗画像': 'Flag of the United Kingdom.svg',\n",
              " '国歌': '女王陛下万歳|神よ女王陛下を守り給え',\n",
              " '国章リンク': '（イギリスの国章|国章）',\n",
              " '国章画像': 'o',\n",
              " '国際電話番号': '44',\n",
              " '夏時間': '+1',\n",
              " '建国形態': '建国',\n",
              " '日本語国名': 'グレートブリテン及び北アイルランド連合王国',\n",
              " '時間帯': '±0',\n",
              " '最大都市': 'ロンドン',\n",
              " '標語': 'Dieu et mon droit（フランス語:神と私の権利）',\n",
              " '水面積率': '1.3%',\n",
              " '注記': '',\n",
              " '略名': 'イギリス',\n",
              " '確立年月日1': '927年／843年',\n",
              " '確立年月日2': '1707年',\n",
              " '確立年月日3': '1801年',\n",
              " '確立年月日4': '1927年',\n",
              " '確立形態1': 'イングランド王国／スコットランド王国（両国とも連合法 (1707年)|1707年連合法まで）',\n",
              " '確立形態2': 'グレートブリテン王国建国（連合法 (1707年)|1707年連合法）',\n",
              " '確立形態3': 'グレートブリテン及びアイルランド連合王国建国（連合法 (1800年)|1800年連合法）',\n",
              " '確立形態4': '現在の国号「グレートブリテン及び北アイルランド連合王国」に変更',\n",
              " '通貨': 'スターリング・ポンド|UKポンド (&pound;)',\n",
              " '通貨コード': 'GBP',\n",
              " '面積値': '244,820',\n",
              " '面積大きさ': '1 E11',\n",
              " '面積順位': '76',\n",
              " '首相等氏名': 'デーヴィッド・キャメロン',\n",
              " '首相等肩書': 'イギリスの首相|首相',\n",
              " '首都': 'ロンドン'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2pSeEzdAAMR",
        "colab_type": "text"
      },
      "source": [
        "## 29. 国旗画像のURLを取得する\n",
        "テンプレートの内容を利用し，国旗画像のURLを取得せよ．（ヒント: MediaWiki APIのimageinfoを呼び出して，ファイル参照をURLに変換すればよい）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDJjkmaG_JdI",
        "colab_type": "code",
        "outputId": "2a62e608-af2e-45e6-f4d9-cb6365671a25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import requests\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "\n",
        "def get_uk_data(wiki_data):\n",
        "    for i in wiki_data:\n",
        "        title = json.loads(i)[\"title\"]\n",
        "        if title == \"イギリス\":\n",
        "            return json.loads(i)[\"text\"]\n",
        "        \n",
        "def is_invalid_word(target_data):\n",
        "    if \" \" not in target_data or \"=\" not in target_data:\n",
        "        return False\n",
        "    elif \"基礎情報\" in target_data:\n",
        "        return False\n",
        "    return True\n",
        "    \n",
        "def to_dict(sentence):\n",
        "    d = {}\n",
        "    for data in sentence.split(\"\\n\"):\n",
        "        if data == \"}}\":\n",
        "            break\n",
        "        if not is_invalid_word(data):\n",
        "            continue\n",
        "        key = data.split(\"=\")[0].replace(\"|\", \"\").strip()\n",
        "        val = re.sub(\"'|\\[\\[|\\]\\]\", \"\", data.split(\"=\")[1]).split(\"<ref\")[0].strip()\n",
        "        val = re.sub(\"<br.*>\", \"\", val)\n",
        "        if \"{{\" in val:\n",
        "            val = re.sub(\"\\{\\{|\\}\\}\", \"\", val).split(\"|\")[-1]\n",
        "        if \"ファイル:\" in val:\n",
        "            val = val.replace(\"ファイル:\", \"\")[1].split(\"|\")[0]\n",
        "        d[key] = val\n",
        "    return d\n",
        "            \n",
        "def request_to_wiki(image_file_name):\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "    \"action\":\"query\",\n",
        "    \"format\":\"json\",\n",
        "    \"prop\": \"imageinfo\",\n",
        "    \"titles\":\"File:{}\".format(image_file_name),\n",
        "    \"iiprop\": \"url\",\n",
        "    }\n",
        "    header = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    time.sleep(3)\n",
        "    return requests.get(url, headers=header, params=params)\n",
        "\n",
        "def get_national_flag(image_file_name):\n",
        "    try:\n",
        "        req = request_to_wiki(image_file_name)\n",
        "        if req.status_code != 200:\n",
        "            raise Exception(\"Wiki APIへのリクエストが失敗しました\")\n",
        "        return req.json()[\"query\"][\"pages\"][\"23473560\"]['imageinfo'][0]['url']\n",
        "    except:\n",
        "        raise\n",
        "\n",
        "with gzip.open(\"/content/drive/My Drive/jawiki-country.json.gz\", mode=\"rt\") as f:\n",
        "    wiki_data = f.readlines()\n",
        "uk_data = get_uk_data(wiki_data)\n",
        "dict_data = to_dict(uk_data)\n",
        "national_flag_url = get_national_flag(dict_data[\"国旗画像\"])\n",
        "print(national_flag_url)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://upload.wikimedia.org/wikipedia/en/a/ae/Flag_of_the_United_Kingdom.svg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZsyZX9W7KyN",
        "colab_type": "text"
      },
      "source": [
        "# 第4章: 形態素解析\n",
        "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をMeCabを使って形態素解析し，その結果をneko.txt.mecabというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
        "\n",
        "なお，問題37, 38, 39はmatplotlibもしくはGnuplotを用いるとよい．\n",
        "\n",
        "## 30. 形態素解析結果の読み込み\n",
        "形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ．ただし，各形態素は表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をキーとするマッピング型に格納し，1文を形態素（マッピング型）のリストとして表現せよ．第4章の残りの問題では，ここで作ったプログラムを活用せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrq9T2rp7Kld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install mecab\n",
        "!apt install libmecab-dev\n",
        "!apt install mecab-ipadic-utf8\n",
        "\n",
        "!git clone https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!cd mecab-ipadic-neologd && bin/install-mecab-ipadic-neologd\n",
        "!echo \"; dicdir = /var/lib/mecab/dic/debian \\ndicdir = /usr/lib/mecab/dic/mecab-ipadic-neologd\" > /etc/mecabrc\n",
        "\n",
        "!apt-get install mecab mecab-ipadic-utf8 libmecab-dev swig\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5ToePVf7mM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "\n",
        "import MeCab\n",
        "\n",
        "with codecs.open(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt\", \"r\", \"utf-8\") as f:\n",
        "    neko_data = f.read()\n",
        " \n",
        "t = MeCab.Tagger('')\n",
        "with codecs.open(\"neko.txt.mecab\", \"w\", \"utf-8\") as out_f:\n",
        "    out_f.write(t.parse(neko_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj3p36GfAWwj",
        "colab_type": "code",
        "outputId": "8c93492f-cd44-4a44-b9ad-533dda779f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import codecs\n",
        "\n",
        "import MeCab\n",
        "\n",
        "\n",
        "class MeCabAnalyzer:\n",
        "# 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音 \n",
        "# 一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger(\"\")\n",
        "    \n",
        "    def morpheme_to_dict(self, doc):\n",
        "        morphemes = doc.splitlines()[0].split(\",\")\n",
        "        return {\"surface\": morphemes[0].split(\"\\t\")[0],\n",
        "                \"base\": morphemes[6],\n",
        "                \"pos\": morphemes[0].split(\"\\t\")[1],\n",
        "                \"pos1\": morphemes[1],\n",
        "        }\n",
        "\n",
        "    def parser(self, doc):\n",
        "        return self.tagger.parse(doc)\n",
        "\n",
        "    def to_sentence_list(self, morpheme_list):\n",
        "        result = []\n",
        "        sentence = []\n",
        "        for morpheme in morpheme_list:\n",
        "            sentence.append(morpheme)\n",
        "            if morpheme[\"pos1\"] == \"句点\":\n",
        "                result.append(sentence)\n",
        "                sentence = []\n",
        "        return result\n",
        "\n",
        "    \n",
        "analyzer = MeCabAnalyzer()\n",
        "with codecs.open(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.mecab\", \"r\", \"utf-8\") as f:\n",
        "    node = analyzer.parser(f.readline())\n",
        "    result = []\n",
        "    while node:\n",
        "        if len(node.split(\"\\n\")) <= 3:\n",
        "            break\n",
        "        result.append(analyzer.morpheme_to_dict(node))\n",
        "        node = analyzer.parser(f.readline())\n",
        "\n",
        "sentence_list = analyzer.to_sentence_list(result)\n",
        "print(sentence_list[0])"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'surface': '一', 'base': '一', 'pos': '名詞', 'pos1': '数'}, {'surface': '\\u3000', 'base': '\\u3000', 'pos': '記号', 'pos1': '空白'}, {'surface': '吾輩', 'base': '吾輩', 'pos': '名詞', 'pos1': '代名詞'}, {'surface': 'は', 'base': 'は', 'pos': '助詞', 'pos1': '係助詞'}, {'surface': '猫', 'base': '猫', 'pos': '名詞', 'pos1': '一般'}, {'surface': 'で', 'base': 'で', 'pos': '接続詞', 'pos1': '*'}, {'surface': 'ある', 'base': 'ある', 'pos': '連体詞', 'pos1': '*'}, {'surface': '。', 'base': '。', 'pos': '記号', 'pos1': '句点'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0t85qHwek4t",
        "colab_type": "text"
      },
      "source": [
        "## 31. 動詞\n",
        "動詞の表層形をすべて抽出せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldaE6q2wXKA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68579e7f-a0f9-4e47-ff9a-76b8303cc2f6"
      },
      "source": [
        "import codecs\n",
        "\n",
        "import MeCab\n",
        "\n",
        "\n",
        "class MeCabAnalyzer:\n",
        "# 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音 \n",
        "# 一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger(\"\")\n",
        "    \n",
        "    def morpheme_to_dict(self, doc):\n",
        "        morphemes = doc.splitlines()[0].split(\",\")\n",
        "        return {\"surface\": morphemes[0].split(\"\\t\")[0],\n",
        "                \"base\": morphemes[6],\n",
        "                \"pos\": morphemes[0].split(\"\\t\")[1],\n",
        "                \"pos1\": morphemes[1],\n",
        "        }\n",
        "\n",
        "    def parser(self, doc):\n",
        "        return self.tagger.parse(doc)\n",
        "\n",
        "    def to_sentence_list(self, morpheme_list):\n",
        "        result = []\n",
        "        sentence = []\n",
        "        for morpheme in morpheme_list:\n",
        "            sentence.append(morpheme)\n",
        "            if morpheme[\"pos1\"] == \"句点\":\n",
        "                result.append(sentence)\n",
        "                sentence = []\n",
        "        return result\n",
        "\n",
        "    def get_morpheme(self, morpheme_list, target_morpheme, target_data):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                result.append(morpheme)\n",
        "        return result\n",
        "    \n",
        "\n",
        "analyzer = MeCabAnalyzer()\n",
        "with codecs.open(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.mecab\", \"r\", \"utf-8\") as f:\n",
        "    node = analyzer.parser(f.readline())\n",
        "    result = []\n",
        "    while node:\n",
        "        if len(node.split(\"\\n\")) <= 3:\n",
        "            break\n",
        "        result.append(analyzer.morpheme_to_dict(node))\n",
        "        node = analyzer.parser(f.readline())\n",
        "\n",
        "verb_list = analyzer.get_morpheme(result, \"pos\", \"動詞\")\n",
        "print(verb_list[0])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'surface': 'つか', 'base': 'つく', 'pos': '動詞', 'pos1': '自立'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cxasaDkhF6H",
        "colab_type": "text"
      },
      "source": [
        "## 32. 動詞の原形\n",
        "動詞の原形をすべて抽出せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHfyA-AYhFiJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d09791e-05de-464d-b56e-9e4764f357a3"
      },
      "source": [
        "import codecs\n",
        "\n",
        "import MeCab\n",
        "\n",
        "\n",
        "class MeCabAnalyzer:\n",
        "# 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音 \n",
        "# 一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger(\"\")\n",
        "    \n",
        "    def morpheme_to_dict(self, doc):\n",
        "        morphemes = doc.splitlines()[0].split(\",\")\n",
        "        return {\"surface\": morphemes[0].split(\"\\t\")[0],\n",
        "                \"base\": morphemes[6],\n",
        "                \"pos\": morphemes[0].split(\"\\t\")[1],\n",
        "                \"pos1\": morphemes[1],\n",
        "        }\n",
        "\n",
        "    def parser(self, doc):\n",
        "        return self.tagger.parse(doc)\n",
        "\n",
        "    def to_sentence_list(self, morpheme_list):\n",
        "        result = []\n",
        "        sentence = []\n",
        "        for morpheme in morpheme_list:\n",
        "            sentence.append(morpheme)\n",
        "            if morpheme[\"pos1\"] == \"句点\":\n",
        "                result.append(sentence)\n",
        "                sentence = []\n",
        "        return result\n",
        "\n",
        "    def get_morpheme(self, morpheme_list, target_morpheme, target_data):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                result.append(morpheme)\n",
        "        return result\n",
        "    \n",
        "    def get_morpheme_data(self, morpheme_list, target_morpheme):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            result.append(morpheme[target_morpheme])\n",
        "        return result\n",
        "    \n",
        "\n",
        "\n",
        "analyzer = MeCabAnalyzer()\n",
        "with codecs.open(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.mecab\", \"r\", \"utf-8\") as f:\n",
        "    node = analyzer.parser(f.readline())\n",
        "    result = []\n",
        "    while node:\n",
        "        if len(node.split(\"\\n\")) <= 3:\n",
        "            break\n",
        "        result.append(analyzer.morpheme_to_dict(node))\n",
        "        node = analyzer.parser(f.readline())\n",
        "\n",
        "verb_list = analyzer.get_morpheme(result, \"pos\", \"動詞\")\n",
        "verb_base_list = analyzer.get_morpheme_data(verb_list, \"base\")\n",
        "print(verb_base_list[0])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "つく\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xHoKoCpjI1t",
        "colab_type": "text"
      },
      "source": [
        "## 33. サ変名詞\n",
        "サ変接続の名詞をすべて抽出せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adKHNw7_jIfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a096ee17-284a-4630-e371-8752a77d01de"
      },
      "source": [
        "import codecs\n",
        "\n",
        "import MeCab\n",
        "\n",
        "\n",
        "class MeCabAnalyzer:\n",
        "# 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音 \n",
        "# 一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger(\"\")\n",
        "    \n",
        "    def morpheme_to_dict(self, doc):\n",
        "        morphemes = doc.splitlines()[0].split(\",\")\n",
        "        return {\"surface\": morphemes[0].split(\"\\t\")[0],\n",
        "                \"base\": morphemes[6],\n",
        "                \"pos\": morphemes[0].split(\"\\t\")[1],\n",
        "                \"pos1\": morphemes[1],\n",
        "        }\n",
        "\n",
        "    def parser(self, doc):\n",
        "        return self.tagger.parse(doc)\n",
        "\n",
        "    def to_sentence_list(self, morpheme_list):\n",
        "        result = []\n",
        "        sentence = []\n",
        "        for morpheme in morpheme_list:\n",
        "            sentence.append(morpheme)\n",
        "            if morpheme[\"pos1\"] == \"句点\":\n",
        "                result.append(sentence)\n",
        "                sentence = []\n",
        "        return result\n",
        "\n",
        "    def get_morpheme(self, morpheme_list, target_morpheme, target_data):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                result.append(morpheme)\n",
        "        return result\n",
        "    \n",
        "    def get_morpheme_data(self, morpheme_list, target_morpheme):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            result.append(morpheme[target_morpheme])\n",
        "        return result\n",
        "    \n",
        "\n",
        "\n",
        "analyzer = MeCabAnalyzer()\n",
        "with codecs.open(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.mecab\", \"r\", \"utf-8\") as f:\n",
        "    node = analyzer.parser(f.readline())\n",
        "    result = []\n",
        "    while node:\n",
        "        if len(node.split(\"\\n\")) <= 3:\n",
        "            break\n",
        "        result.append(analyzer.morpheme_to_dict(node))\n",
        "        node = analyzer.parser(f.readline())\n",
        "\n",
        "noun = analyzer.get_morpheme(result, \"pos1\", \"サ変接続\")\n",
        "noun[0]"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'base': '見当', 'pos': '名詞', 'pos1': 'サ変接続', 'surface': '見当'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnXKP_PpkfKh",
        "colab_type": "text"
      },
      "source": [
        "## 34. 「AのB」\n",
        "2つの名詞が「の」で連結されている名詞句を抽出せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hifP8Wbpke0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17aaf798-0080-422c-88c9-55cdf5663843"
      },
      "source": [
        "import codecs\n",
        "\n",
        "import MeCab\n",
        "\n",
        "\n",
        "class MeCabAnalyzer:\n",
        "# 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音 \n",
        "# 一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger(\"\")\n",
        "    \n",
        "    def morpheme_to_dict(self, doc):\n",
        "        morphemes = doc.splitlines()[0].split(\",\")\n",
        "        return {\"surface\": morphemes[0].split(\"\\t\")[0],\n",
        "                \"base\": morphemes[6],\n",
        "                \"pos\": morphemes[0].split(\"\\t\")[1],\n",
        "                \"pos1\": morphemes[1],\n",
        "        }\n",
        "\n",
        "    def parser(self, doc):\n",
        "        return self.tagger.parse(doc)\n",
        "\n",
        "    def to_sentence_list(self, morpheme_list):\n",
        "        result = []\n",
        "        sentence = []\n",
        "        for morpheme in morpheme_list:\n",
        "            sentence.append(morpheme)\n",
        "            if morpheme[\"pos1\"] == \"句点\":\n",
        "                result.append(sentence)\n",
        "                sentence = []\n",
        "        return result\n",
        "\n",
        "    def get_morpheme(self, morpheme_list, target_morpheme, target_data):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                result.append(morpheme)\n",
        "        return result\n",
        "    \n",
        "    def get_morpheme_data(self, morpheme_list, target_morpheme):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            result.append(morpheme[target_morpheme])\n",
        "        return result\n",
        "    \n",
        "    def get_noun_phrase(self, morpheme_list):\n",
        "        result = []\n",
        "        for ind, morpheme in enumerate(morpheme_list):\n",
        "            if morpheme[\"pos\"] == \"名詞\" and morpheme_list[ind + 1][\"surface\"] == \"の\" \\\n",
        "                and morpheme_list[ind + 2][\"pos\"] == \"名詞\":\n",
        "                result.append(\n",
        "                    morpheme[\"surface\"] \\\n",
        "                    + morpheme_list[ind + 1][\"surface\"] \\\n",
        "                    + morpheme_list[ind + 2][\"surface\"]\n",
        "                )\n",
        "        return result\n",
        "        \n",
        "\n",
        "\n",
        "analyzer = MeCabAnalyzer()\n",
        "with codecs.open(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.mecab\", \"r\", \"utf-8\") as f:\n",
        "    node = analyzer.parser(f.readline())\n",
        "    result = []\n",
        "    while node:\n",
        "        if len(node.split(\"\\n\")) <= 3:\n",
        "            break\n",
        "        result.append(analyzer.morpheme_to_dict(node))\n",
        "        node = analyzer.parser(f.readline())\n",
        "\n",
        "noun_phrase = analyzer.get_noun_phrase(result)\n",
        "noun_phrase[0]"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'彼の掌'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R4U1WZMnGcy",
        "colab_type": "text"
      },
      "source": [
        "## 35. 名詞の連接\n",
        "名詞の連接（連続して出現する名詞）を最長一致で抽出せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHEtjwawnGDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b22b738a-e942-4d2c-d13c-95bd81be53c7"
      },
      "source": [
        "import codecs\n",
        "\n",
        "import MeCab\n",
        "\n",
        "\n",
        "class MeCabAnalyzer:\n",
        "# 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音 \n",
        "# 一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger(\"\")\n",
        "    \n",
        "    def morpheme_to_dict(self, doc):\n",
        "        morphemes = doc.splitlines()[0].split(\",\")\n",
        "        return {\"surface\": morphemes[0].split(\"\\t\")[0],\n",
        "                \"base\": morphemes[6],\n",
        "                \"pos\": morphemes[0].split(\"\\t\")[1],\n",
        "                \"pos1\": morphemes[1],\n",
        "        }\n",
        "\n",
        "    def parser(self, doc):\n",
        "        return self.tagger.parse(doc)\n",
        "\n",
        "    def to_sentence_list(self, morpheme_list):\n",
        "        result = []\n",
        "        sentence = []\n",
        "        for morpheme in morpheme_list:\n",
        "            sentence.append(morpheme)\n",
        "            if morpheme[\"pos1\"] == \"句点\":\n",
        "                result.append(sentence)\n",
        "                sentence = []\n",
        "        return result\n",
        "\n",
        "    def get_morpheme(self, morpheme_list, target_morpheme, target_data):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                result.append(morpheme)\n",
        "        return result\n",
        "    \n",
        "    def get_morpheme_data(self, morpheme_list, target_morpheme):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            result.append(morpheme[target_morpheme])\n",
        "        return result\n",
        "    \n",
        "    def get_noun_phrase(self, morpheme_list):\n",
        "        result = []\n",
        "        for ind, morpheme in enumerate(morpheme_list):\n",
        "            if morpheme[\"pos\"] == \"名詞\" and morpheme_list[ind + 1][\"surface\"] == \"の\" \\\n",
        "                and morpheme_list[ind + 2][\"pos\"] == \"名詞\":\n",
        "                result.append(\n",
        "                    morpheme[\"surface\"] \\\n",
        "                    + morpheme_list[ind + 1][\"surface\"] \\\n",
        "                    + morpheme_list[ind + 2][\"surface\"]\n",
        "                )\n",
        "        return result\n",
        "        \n",
        "    def get_most_continuous_morpheme_data(self, morpheme_list, target_morpheme, target_data):\n",
        "        cnt = 0\n",
        "        result_word = {}\n",
        "        tmp_word = \"\"\n",
        "        for  morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                cnt += 1\n",
        "                tmp_word += morpheme[\"surface\"]\n",
        "            else:\n",
        "                if cnt <= 0:\n",
        "                    continue\n",
        "                if cnt in result_word:\n",
        "                    result_word[cnt].append(tmp_word)\n",
        "                else:\n",
        "                    result_word[cnt] = [tmp_word]\n",
        "                cnt = 0\n",
        "                tmp_word = \"\"\n",
        "        return result_word[max(result_word.keys())]\n",
        "\n",
        "\n",
        "analyzer = MeCabAnalyzer()\n",
        "with codecs.open(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.mecab\", \"r\", \"utf-8\") as f:\n",
        "    node = analyzer.parser(f.readline())\n",
        "    result = []\n",
        "    while node:\n",
        "        if len(node.split(\"\\n\")) <= 3:\n",
        "            break\n",
        "        result.append(analyzer.morpheme_to_dict(node))\n",
        "        node = analyzer.parser(f.readline())\n",
        "\n",
        "most_continuous_words = analyzer.get_most_continuous_morpheme_data(result, \"pos\", \"名詞\")\n",
        "most_continuous_words"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"manyaslip'twixtthecupandthelip\", '明治三十八年何月何日戸締り']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiKTPq93vSDW",
        "colab_type": "text"
      },
      "source": [
        "## 36. 単語の出現頻度\n",
        "文章中に出現する単語とその出現頻度を求め，出現頻度の高い順に並べよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdv8OvJZoCHe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b610034-e8f8-4876-f8d5-8b13ca14c229"
      },
      "source": [
        "import codecs\n",
        "import collections\n",
        "\n",
        "import MeCab\n",
        "\n",
        "\n",
        "class MeCabAnalyzer:\n",
        "# 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音 \n",
        "# 一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger(\"\")\n",
        "    \n",
        "    def morpheme_to_dict(self, doc):\n",
        "        morphemes = doc.splitlines()[0].split(\",\")\n",
        "        return {\"surface\": morphemes[0].split(\"\\t\")[0],\n",
        "                \"base\": morphemes[6],\n",
        "                \"pos\": morphemes[0].split(\"\\t\")[1],\n",
        "                \"pos1\": morphemes[1],\n",
        "        }\n",
        "\n",
        "    def parser(self, doc):\n",
        "        return self.tagger.parse(doc)\n",
        "\n",
        "    def to_sentence_list(self, morpheme_list):\n",
        "        result = []\n",
        "        sentence = []\n",
        "        for morpheme in morpheme_list:\n",
        "            sentence.append(morpheme)\n",
        "            if morpheme[\"pos1\"] == \"句点\":\n",
        "                result.append(sentence)\n",
        "                sentence = []\n",
        "        return result\n",
        "\n",
        "    def get_morpheme(self, morpheme_list, target_morpheme, target_data):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                result.append(morpheme)\n",
        "        return result\n",
        "    \n",
        "    def get_morpheme_data(self, morpheme_list, target_morpheme):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            result.append(morpheme[target_morpheme])\n",
        "        return result\n",
        "    \n",
        "    def get_noun_phrase(self, morpheme_list):\n",
        "        result = []\n",
        "        for ind, morpheme in enumerate(morpheme_list):\n",
        "            if morpheme[\"pos\"] == \"名詞\" and morpheme_list[ind + 1][\"surface\"] == \"の\" \\\n",
        "                and morpheme_list[ind + 2][\"pos\"] == \"名詞\":\n",
        "                result.append(\n",
        "                    morpheme[\"surface\"] \\\n",
        "                    + morpheme_list[ind + 1][\"surface\"] \\\n",
        "                    + morpheme_list[ind + 2][\"surface\"]\n",
        "                )\n",
        "        return result\n",
        "        \n",
        "    def get_most_continuous_morpheme_data(self, morpheme_list, target_morpheme, target_data):\n",
        "        cnt = 0\n",
        "        result_word = {}\n",
        "        tmp_word = \"\"\n",
        "        for  morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                cnt += 1\n",
        "                tmp_word += morpheme[\"surface\"]\n",
        "            else:\n",
        "                if cnt <= 0:\n",
        "                    continue\n",
        "                if cnt in result_word:\n",
        "                    result_word[cnt].append(tmp_word)\n",
        "                else:\n",
        "                    result_word[cnt] = [tmp_word]\n",
        "                cnt = 0\n",
        "                tmp_word = \"\"\n",
        "        return result_word[max(result_word.keys())]\n",
        "\n",
        "    def mapping_word_flequency(self, morpheme_list):\n",
        "        return collections.Counter([x[\"surface\"] for x in morpheme_list])\n",
        "\n",
        "\n",
        "analyzer = MeCabAnalyzer()\n",
        "with codecs.open(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.mecab\", \"r\", \"utf-8\") as f:\n",
        "    node = analyzer.parser(f.readline())\n",
        "    result = []\n",
        "    while node:\n",
        "        if len(node.split(\"\\n\")) <= 3:\n",
        "            break\n",
        "        result.append(analyzer.morpheme_to_dict(node))\n",
        "        node = analyzer.parser(f.readline())\n",
        "\n",
        "most_continuous_words = analyzer.mapping_word_flequency(result)\n",
        "most_continuous_words.most_common()[:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('の', 9195), ('。', 7486), ('て', 6873), ('、', 6772), ('は', 6430)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qmGjy4zzN5B",
        "colab_type": "text"
      },
      "source": [
        "## 37. 頻度上位10語\n",
        "出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4IY3kKKzNi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "7283a5ab-2f1b-4764-e572-4e54da7b97c2"
      },
      "source": [
        "import codecs\n",
        "import collections\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import MeCab\n",
        "\n",
        "\n",
        "class MeCabAnalyzer:\n",
        "# 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音 \n",
        "# 一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger(\"\")\n",
        "    \n",
        "    def morpheme_to_dict(self, doc):\n",
        "        morphemes = doc.splitlines()[0].split(\",\")\n",
        "        return {\"surface\": morphemes[0].split(\"\\t\")[0],\n",
        "                \"base\": morphemes[6],\n",
        "                \"pos\": morphemes[0].split(\"\\t\")[1],\n",
        "                \"pos1\": morphemes[1],\n",
        "        }\n",
        "\n",
        "    def parser(self, doc):\n",
        "        return self.tagger.parse(doc)\n",
        "\n",
        "    def to_sentence_list(self, morpheme_list):\n",
        "        result = []\n",
        "        sentence = []\n",
        "        for morpheme in morpheme_list:\n",
        "            sentence.append(morpheme)\n",
        "            if morpheme[\"pos1\"] == \"句点\":\n",
        "                result.append(sentence)\n",
        "                sentence = []\n",
        "        return result\n",
        "\n",
        "    def get_morpheme(self, morpheme_list, target_morpheme, target_data):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                result.append(morpheme)\n",
        "        return result\n",
        "    \n",
        "    def get_morpheme_data(self, morpheme_list, target_morpheme):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            result.append(morpheme[target_morpheme])\n",
        "        return result\n",
        "    \n",
        "    def get_noun_phrase(self, morpheme_list):\n",
        "        result = []\n",
        "        for ind, morpheme in enumerate(morpheme_list):\n",
        "            if morpheme[\"pos\"] == \"名詞\" and morpheme_list[ind + 1][\"surface\"] == \"の\" \\\n",
        "                and morpheme_list[ind + 2][\"pos\"] == \"名詞\":\n",
        "                result.append(\n",
        "                    morpheme[\"surface\"] \\\n",
        "                    + morpheme_list[ind + 1][\"surface\"] \\\n",
        "                    + morpheme_list[ind + 2][\"surface\"]\n",
        "                )\n",
        "        return result\n",
        "        \n",
        "    def get_most_continuous_morpheme_data(self, morpheme_list, target_morpheme, target_data):\n",
        "        cnt = 0\n",
        "        result_word = {}\n",
        "        tmp_word = \"\"\n",
        "        for  morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                cnt += 1\n",
        "                tmp_word += morpheme[\"surface\"]\n",
        "            else:\n",
        "                if cnt <= 0:\n",
        "                    continue\n",
        "                if cnt in result_word:\n",
        "                    result_word[cnt].append(tmp_word)\n",
        "                else:\n",
        "                    result_word[cnt] = [tmp_word]\n",
        "                cnt = 0\n",
        "                tmp_word = \"\"\n",
        "        return result_word[max(result_word.keys())]\n",
        "\n",
        "    def mapping_word_flequency(self, morpheme_list):\n",
        "        return collections.Counter([x[\"surface\"] for x in morpheme_list])\n",
        "\n",
        "\n",
        "class GraphCreator:\n",
        "\n",
        "    def create_bar_graph(self, data):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.bar(np.arange(1, len(data) + 1), [y[1] for y in data], tick_label=[x[0] for x in data])\n",
        "        return plt\n",
        "\n",
        "\n",
        "analyzer = MeCabAnalyzer()\n",
        "with codecs.open(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.mecab\", \"r\", \"utf-8\") as f:\n",
        "    node = analyzer.parser(f.readline())\n",
        "    result = []\n",
        "    while node:\n",
        "        if len(node.split(\"\\n\")) <= 3:\n",
        "            break\n",
        "        result.append(analyzer.morpheme_to_dict(node))\n",
        "        node = analyzer.parser(f.readline())\n",
        "\n",
        "most_continuous_words = analyzer.mapping_word_flequency(result)\n",
        "graph_creator = GraphCreator()\n",
        "graph_creator.create_bar_graph(most_continuous_words.most_common()[:10]).show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1JQBkKSmDIAkNWUAJKmp/wVtE27LvZuwuwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_FKIcst52_I",
        "colab_type": "text"
      },
      "source": [
        "## 38. ヒストグラム\n",
        "単語の出現頻度のヒストグラム（横軸に出現頻度，縦軸に出現頻度をとる単語の種類数を棒グラフで表したもの）を描け．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUohRlyE52vs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "51295475-e6a4-4c03-8afd-3bde2c9c9554"
      },
      "source": [
        "import codecs\n",
        "import collections\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import MeCab\n",
        "\n",
        "\n",
        "class MeCabAnalyzer:\n",
        "# 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音 \n",
        "# 一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger(\"\")\n",
        "    \n",
        "    def morpheme_to_dict(self, doc):\n",
        "        morphemes = doc.splitlines()[0].split(\",\")\n",
        "        return {\"surface\": morphemes[0].split(\"\\t\")[0],\n",
        "                \"base\": morphemes[6],\n",
        "                \"pos\": morphemes[0].split(\"\\t\")[1],\n",
        "                \"pos1\": morphemes[1],\n",
        "        }\n",
        "\n",
        "    def parser(self, doc):\n",
        "        return self.tagger.parse(doc)\n",
        "\n",
        "    def to_sentence_list(self, morpheme_list):\n",
        "        result = []\n",
        "        sentence = []\n",
        "        for morpheme in morpheme_list:\n",
        "            sentence.append(morpheme)\n",
        "            if morpheme[\"pos1\"] == \"句点\":\n",
        "                result.append(sentence)\n",
        "                sentence = []\n",
        "        return result\n",
        "\n",
        "    def get_morpheme(self, morpheme_list, target_morpheme, target_data):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                result.append(morpheme)\n",
        "        return result\n",
        "    \n",
        "    def get_morpheme_data(self, morpheme_list, target_morpheme):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            result.append(morpheme[target_morpheme])\n",
        "        return result\n",
        "    \n",
        "    def get_noun_phrase(self, morpheme_list):\n",
        "        result = []\n",
        "        for ind, morpheme in enumerate(morpheme_list):\n",
        "            if morpheme[\"pos\"] == \"名詞\" and morpheme_list[ind + 1][\"surface\"] == \"の\" \\\n",
        "                and morpheme_list[ind + 2][\"pos\"] == \"名詞\":\n",
        "                result.append(\n",
        "                    morpheme[\"surface\"] \\\n",
        "                    + morpheme_list[ind + 1][\"surface\"] \\\n",
        "                    + morpheme_list[ind + 2][\"surface\"]\n",
        "                )\n",
        "        return result\n",
        "        \n",
        "    def get_most_continuous_morpheme_data(self, morpheme_list, target_morpheme, target_data):\n",
        "        cnt = 0\n",
        "        result_word = {}\n",
        "        tmp_word = \"\"\n",
        "        for  morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                cnt += 1\n",
        "                tmp_word += morpheme[\"surface\"]\n",
        "            else:\n",
        "                if cnt <= 0:\n",
        "                    continue\n",
        "                if cnt in result_word:\n",
        "                    result_word[cnt].append(tmp_word)\n",
        "                else:\n",
        "                    result_word[cnt] = [tmp_word]\n",
        "                cnt = 0\n",
        "                tmp_word = \"\"\n",
        "        return result_word[max(result_word.keys())]\n",
        "\n",
        "    def mapping_word_flequency(self, morpheme_list):\n",
        "        return collections.Counter([x[\"surface\"] for x in morpheme_list])\n",
        "\n",
        "\n",
        "class GraphCreator:\n",
        "\n",
        "    def create_bar_graph(self, data):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.bar(np.arange(1, len(data) + 1), [y[1] for y in data], tick_label=[x[0] for x in data])\n",
        "        return plt\n",
        "\n",
        "    def create_histogram(self, data):\n",
        "        fig, ax = plt.subplots()\n",
        "        plt.xlim(xmin=1, xmax=20)\n",
        "        n, bins, patches = ax.hist([y[1] for y in data], bins=20, range=(1,20))\n",
        "        return plt\n",
        "\n",
        "\n",
        "analyzer = MeCabAnalyzer()\n",
        "with codecs.open(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.mecab\", \"r\", \"utf-8\") as f:\n",
        "    node = analyzer.parser(f.readline())\n",
        "    result = []\n",
        "    while node:\n",
        "        if len(node.split(\"\\n\")) <= 3:\n",
        "            break\n",
        "        result.append(analyzer.morpheme_to_dict(node))\n",
        "        node = analyzer.parser(f.readline())\n",
        "\n",
        "most_continuous_words = analyzer.mapping_word_flequency(result)\n",
        "graph_creator = GraphCreator()\n",
        "graph_creator.create_histogram(most_continuous_words.most_common()).show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py:3215: MatplotlibDeprecationWarning: \n",
            "The `xmin` argument was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use `left` instead.\n",
            "  alternative='`left`', obj_type='argument')\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py:3221: MatplotlibDeprecationWarning: \n",
            "The `xmax` argument was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use `right` instead.\n",
            "  alternative='`right`', obj_type='argument')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEv9JREFUeJzt3X/sXfV93/Hnq3ZIS5rZJrSeZ1sz\nXa1UpFISZBGyZlEWVmOcKKZTi6iq4VBLViUyJVKn1lmn0uWHBJvWLJlWJhq8migLsLQpVkJLPCdR\ntT8gGEIgQDJ/oSBsGbzGjtPMajLc9/64H9PLl8/X3/vF93u/1/PzIV3dcz7nc859n+Pj+/Ln3HOv\nU1VIkjTbjy11AZKk6WRASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktS1fKkLOJOL\nL764NmzYsNRlSNI55aGHHvqrqvqps93OVAfEhg0bOHDgwFKXIUnnlCTPjmM7XmKSJHUZEJKkLgNC\nktRlQEiSugwISVKXASFJ6jIgJEldIwVEkpVJPp/k20meTPL2JBcl2ZfkYHte1fomyaeSzCR5NMll\nQ9vZ3vofTLJ9sXZKknT2Rh1BfBL486r6OeDNwJPALmB/VW0E9rd5gKuBje2xE7gVIMlFwE3A24DL\ngZtOh4okafrM+03qJCuAdwLvB6iqHwE/SrINeFfrtgf4GvDbwDbgjqoq4P42+ljT+u6rqmNtu/uA\nLcDn5nrtxw6fYMOuL72a/XrJMze/56zWl6Tz1SgjiEuA/w381yTfSPLpJK8DVlfVkdbneWB1m14L\nPDe0/qHWNle7JGkKjRIQy4HLgFur6q3A/+HvLicB0EYLNY6CkuxMciDJgVMnT4xjk5KkV2GUgDgE\nHKqqB9r85xkExgvt0hHt+WhbfhhYP7T+utY2V/vLVNVtVbWpqjYtu3DFQvZFkjRG8wZEVT0PPJfk\nja3pSuAJYC9w+k6k7cA9bXovcH27m+kK4ES7FHUfsDnJqvbh9ObWJkmaQqP+3Pe/BD6b5ALgaeAG\nBuFyd5IdwLPAta3vvcBWYAY42fpSVceSfBR4sPX7yOkPrCVJ02ekgKiqR4BNnUVXdvoWcOMc29kN\n7F5IgZKkpeE3qSVJXQaEJKnLgJAkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQ\nJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElS\nlwEhSeoyICRJXQaEJKlrpIBI8kySx5I8kuRAa7soyb4kB9vzqtaeJJ9KMpPk0SSXDW1ne+t/MMn2\nxdklSdI4LGQE8U+r6i1VtanN7wL2V9VGYH+bB7ga2NgeO4FbYRAowE3A24DLgZtOh4okafqczSWm\nbcCeNr0HuGao/Y4auB9YmWQNcBWwr6qOVdVxYB+w5SxeX5K0iEYNiAK+nOShJDtb2+qqOtKmnwdW\nt+m1wHND6x5qbXO1v0ySnUkOJDlw6uSJEcuTJI3b8hH7vaOqDif5aWBfkm8PL6yqSlLjKKiqbgNu\nA3jtmo1j2aYkaeFGGkFU1eH2fBT4AoPPEF5ol45oz0db98PA+qHV17W2udolSVNo3oBI8rokrz89\nDWwGvgXsBU7fibQduKdN7wWub3czXQGcaJei7gM2J1nVPpze3NokSVNolEtMq4EvJDnd/79V1Z8n\neRC4O8kO4Fng2tb/XmArMAOcBG4AqKpjST4KPNj6faSqjo1tTyRJYzVvQFTV08CbO+3fBa7stBdw\n4xzb2g3sXniZkqRJ85vUkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNC\nktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJ\nXQaEJKnLgJAkdRkQkqSukQMiybIk30jyxTZ/SZIHkswkuSvJBa39tW1+pi3fMLSND7f27yS5atw7\nI0kan4WMID4IPDk0fwvwiar6WeA4sKO17wCOt/ZPtH4kuRS4DngTsAX4gyTLzq58SdJiGSkgkqwD\n3gN8us0HeDfw+dZlD3BNm97W5mnLr2z9twF3VtUPq+ovgRng8nHshCRp/EYdQfxH4LeAv23zbwC+\nV1UvtvlDwNo2vRZ4DqAtP9H6v9TeWeclSXYmOZDkwKmTJxawK5KkcZo3IJK8FzhaVQ9NoB6q6raq\n2lRVm5ZduGISLylJ6lg+Qp9fAN6XZCvw48DfAz4JrEyyvI0S1gGHW//DwHrgUJLlwArgu0Ptpw2v\nI0maMvOOIKrqw1W1rqo2MPiQ+StV9WvAV4Ffbt22A/e06b1tnrb8K1VVrf26dpfTJcBG4Otj2xNJ\n0liNMoKYy28Ddyb5GPAN4PbWfjvwmSQzwDEGoUJVPZ7kbuAJ4EXgxqo6dRavL0laRAsKiKr6GvC1\nNv00nbuQqupvgF+ZY/2PAx9faJGSpMnzm9SSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXQaEJKnL\ngJAkdRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwI\nSVKXASFJ6jIgJEldBoQkqcuAkCR1GRCSpK55AyLJjyf5epJvJnk8yb9t7ZckeSDJTJK7klzQ2l/b\n5mfa8g1D2/pwa/9OkqsWa6ckSWdvlBHED4F3V9WbgbcAW5JcAdwCfKKqfhY4Duxo/XcAx1v7J1o/\nklwKXAe8CdgC/EGSZePcGUnS+MwbEDXwgzb7mvYo4N3A51v7HuCaNr2tzdOWX5kkrf3OqvphVf0l\nMANcPpa9kCSN3UifQSRZluQR4CiwD3gK+F5Vvdi6HALWtum1wHMAbfkJ4A3D7Z11hl9rZ5IDSQ6c\nOnli4XskSRqLkQKiqk5V1VuAdQz+1f9zi1VQVd1WVZuqatOyC1cs1stIkuaxoLuYqup7wFeBtwMr\nkyxvi9YBh9v0YWA9QFu+AvjucHtnHUnSlBnlLqafSrKyTf8E8IvAkwyC4pdbt+3APW16b5unLf9K\nVVVrv67d5XQJsBH4+rh2RJI0Xsvn78IaYE+74+jHgLur6otJngDuTPIx4BvA7a3/7cBnkswAxxjc\nuURVPZ7kbuAJ4EXgxqo6Nd7dkSSNy7wBUVWPAm/ttD9N5y6kqvob4Ffm2NbHgY8vvExJ0qT5TWpJ\nUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEldBoQkqcuAkCR1\nGRCSpC4DQpLUZUBIkroMCElSlwEhSeoyICRJXQaEJKnLgJAkdS1f6gIW24ZdXzrrbTxz83vGUIkk\nnVscQUiSugwISVKXASFJ6po3IJKsT/LVJE8keTzJB1v7RUn2JTnYnle19iT5VJKZJI8muWxoW9tb\n/4NJti/ebkmSztYoI4gXgd+sqkuBK4Abk1wK7AL2V9VGYH+bB7ga2NgeO4FbYRAowE3A24DLgZtO\nh4okafrMGxBVdaSqHm7Tfw08CawFtgF7Wrc9wDVtehtwRw3cD6xMsga4CthXVceq6jiwD9gy1r2R\nJI3Ngj6DSLIBeCvwALC6qo60Rc8Dq9v0WuC5odUOtba52iVJU2jkgEjyk8AfAx+qqu8PL6uqAmoc\nBSXZmeRAkgOnTp4YxyYlSa/CSAGR5DUMwuGzVfUnrfmFdumI9ny0tR8G1g+tvq61zdX+MlV1W1Vt\nqqpNyy5csZB9kSSN0Sh3MQW4HXiyqn5/aNFe4PSdSNuBe4bar293M10BnGiXou4DNidZ1T6c3tza\nJElTaJSf2vgF4F8AjyV5pLX9a+Bm4O4kO4BngWvbsnuBrcAMcBK4AaCqjiX5KPBg6/eRqjo2lr2Q\nJI3dvAFRVf8TyByLr+z0L+DGOba1G9i9kAIlSUvDb1JLkroMCElSlwEhSeoyICRJXQaEJKnLgJAk\ndRkQkqQuA0KS1GVASJK6DAhJUpcBIUnqMiAkSV0GhCSpy4CQJHUZEJKkLgNCktRlQEiSugwISVKX\nASFJ6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUZUBIkrqWz9chyW7gvcDRqvr51nYRcBewAXgGuLaq\njicJ8ElgK3ASeH9VPdzW2Q78m7bZj1XVnvHuyuLZsOtLZ72NZ25+zxgqkaTJGWUE8UfAllltu4D9\nVbUR2N/mAa4GNrbHTuBWeClQbgLeBlwO3JRk1dkWL0laPPMGRFX9BXBsVvM24PQIYA9wzVD7HTVw\nP7AyyRrgKmBfVR2rquPAPl4ZOpKkKfJqP4NYXVVH2vTzwOo2vRZ4bqjfodY2V7skaUqd9YfUVVVA\njaEWAJLsTHIgyYFTJ0+Ma7OSpAV6tQHxQrt0RHs+2toPA+uH+q1rbXO1v0JV3VZVm6pq07ILV7zK\n8iRJZ+vVBsReYHub3g7cM9R+fQauAE60S1H3AZuTrGofTm9ubZKkKTXKba6fA94FXJzkEIO7kW4G\n7k6yA3gWuLZ1v5fBLa4zDG5zvQGgqo4l+SjwYOv3kaqa/cG3JGmKzBsQVfWrcyy6stO3gBvn2M5u\nYPeCqpMkLRm/SS1J6jIgJEldBoQkqcuAkCR1GRCSpC4DQpLUNe9trhqPs/3JcH8uXNKkOYKQJHUZ\nEJKkLgNCktRlQEiSuvyQ+hzh/4stadIcQUiSugwISVKXASFJ6jIgJEldBoQkqcu7mM4j3gklaSEc\nQUiSuhxBaEEchUjnD0cQkqQuRxCaOH/6XDo3OIKQJHU5gtA5x89BpMkwIHReMmSk+U08IJJsAT4J\nLAM+XVU3T7oGaRzGETLTwKDTXCYaEEmWAf8Z+EXgEPBgkr1V9cQk65D0d6Yh6Ayp6TTpEcTlwExV\nPQ2Q5E5gG2BASOexaQgpvdKk72JaCzw3NH+otUmSpszUfUidZCews83+4Nlb3vudCb30xcBfTei1\nxsF6F5f1Li7rXVxvHMdGJh0Qh4H1Q/PrWttLquo24LZJFgWQ5EBVbZr0675a1ru4rHdxWe/iSnJg\nHNuZ9CWmB4GNSS5JcgFwHbB3wjVIkkYw0RFEVb2Y5APAfQxuc91dVY9PsgZJ0mgm/hlEVd0L3Dvp\n1x3BxC9rnSXrXVzWu7isd3GNpd5U1Ti2I0n6/4w/1idJ6jqvAiLJ+iRfTfJEkseTfLDT511JTiR5\npD1+dylqHarnmSSPtVpecWdCBj6VZCbJo0kuW4o6Wy1vHDpujyT5fpIPzeqzpMc3ye4kR5N8a6jt\noiT7khxsz6vmWHd763MwyfYlrPffJ/l2+/P+QpKVc6x7xnNngvX+XpLDQ3/mW+dYd0uS77RzedcS\n1nvXUK3PJHlkjnWX4vh238MW7RyuqvPmAawBLmvTrwf+F3DprD7vAr641LUO1fMMcPEZlm8F/gwI\ncAXwwFLX3OpaBjwP/MNpOr7AO4HLgG8Ntf07YFeb3gXc0lnvIuDp9ryqTa9aono3A8vb9C29ekc5\ndyZY7+8B/2qE8+Up4GeAC4Bvzv67Oal6Zy3/D8DvTtHx7b6HLdY5fF6NIKrqSFU93Kb/GniSc/+b\n3NuAO2rgfmBlkjVLXRRwJfBUVT271IUMq6q/AI7Nat4G7GnTe4BrOqteBeyrqmNVdRzYB2xZtEKb\nXr1V9eWqerHN3s/g+0RTYY7jO4qXfoanqn4EnP4ZnkV1pnqTBLgW+Nxi1zGqM7yHLco5fF4FxLAk\nG4C3Ag90Fr89yTeT/FmSN020sFcq4MtJHmrfMp9tWn++5Drm/os1TccXYHVVHWnTzwOrO32m9Tj/\nOoMRZM98584kfaBdEts9x+WPaTy+/wR4oaoOzrF8SY/vrPewRTmHz8uASPKTwB8DH6qq789a/DCD\nyyJvBv4T8KeTrm+Wd1TVZcDVwI1J3rnE9cyrfQnyfcB/7yyetuP7MjUYi58Tt/Yl+R3gReCzc3SZ\nlnPnVuAfAW8BjjC4bHMu+FXOPHpYsuN7pvewcZ7D511AJHkNgwP72ar6k9nLq+r7VfWDNn0v8Jok\nF0+4zOF6Drfno8AXGAzFh8378yVL4Grg4ap6YfaCaTu+zQunL8u156OdPlN1nJO8H3gv8GvtDeEV\nRjh3JqKqXqiqU1X1t8AfzlHHtB3f5cA/B+6aq89SHd853sMW5Rw+rwKiXVO8HXiyqn5/jj5/v/Uj\nyeUMjtF3J1fly2p5XZLXn55m8OHkt2Z12wtc3+5mugI4MTTUXCpz/strmo7vkL3A6Ts6tgP3dPrc\nB2xOsqpdItnc2iYug/9067eA91XVyTn6jHLuTMSsz8R+aY46pu1neP4Z8O2qOtRbuFTH9wzvYYtz\nDk/yE/ilfgDvYDD0ehR4pD22Ar8B/Ebr8wHgcQZ3UdwP/OMlrPdnWh3fbDX9TmsfrjcM/hOmp4DH\ngE1LfIxfx+ANf8VQ29QcXwbBdQT4vwyuwe4A3gDsBw4C/wO4qPXdxOB/PTy97q8DM+1xwxLWO8Pg\nWvLpc/i/tL7/ALj3TOfOEtX7mXZuPsrgjWzN7Hrb/FYGd+U8tZT1tvY/On3ODvWdhuM713vYopzD\nfpNaktR1Xl1ikiSNzoCQJHUZEJKkLgNCktRlQEiSugwISVKXASFJ6jIgJEld/w+LV9fPoNmawgAA\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmajdNWp_3W5",
        "colab_type": "text"
      },
      "source": [
        "## 39. Zipfの法則\n",
        "単語の出現頻度順位を横軸，その出現頻度を縦軸として，両対数グラフをプロットせよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbA_kHev6Q2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "import collections\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import MeCab\n",
        "\n",
        "\n",
        "class MeCabAnalyzer:\n",
        "# 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音 \n",
        "# 一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger(\"\")\n",
        "    \n",
        "    def morpheme_to_dict(self, doc):\n",
        "        morphemes = doc.splitlines()[0].split(\",\")\n",
        "        return {\"surface\": morphemes[0].split(\"\\t\")[0],\n",
        "                \"base\": morphemes[6],\n",
        "                \"pos\": morphemes[0].split(\"\\t\")[1],\n",
        "                \"pos1\": morphemes[1],\n",
        "        }\n",
        "\n",
        "    def parser(self, doc):\n",
        "        return self.tagger.parse(doc)\n",
        "\n",
        "    def to_sentence_list(self, morpheme_list):\n",
        "        result = []\n",
        "        sentence = []\n",
        "        for morpheme in morpheme_list:\n",
        "            sentence.append(morpheme)\n",
        "            if morpheme[\"pos1\"] == \"句点\":\n",
        "                result.append(sentence)\n",
        "                sentence = []\n",
        "        return result\n",
        "\n",
        "    def get_morpheme(self, morpheme_list, target_morpheme, target_data):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                result.append(morpheme)\n",
        "        return result\n",
        "    \n",
        "    def get_morpheme_data(self, morpheme_list, target_morpheme):\n",
        "        result = []\n",
        "        for morpheme in morpheme_list:\n",
        "            result.append(morpheme[target_morpheme])\n",
        "        return result\n",
        "    \n",
        "    def get_noun_phrase(self, morpheme_list):\n",
        "        result = []\n",
        "        for ind, morpheme in enumerate(morpheme_list):\n",
        "            if morpheme[\"pos\"] == \"名詞\" and morpheme_list[ind + 1][\"surface\"] == \"の\" \\\n",
        "                and morpheme_list[ind + 2][\"pos\"] == \"名詞\":\n",
        "                result.append(\n",
        "                    morpheme[\"surface\"] \\\n",
        "                    + morpheme_list[ind + 1][\"surface\"] \\\n",
        "                    + morpheme_list[ind + 2][\"surface\"]\n",
        "                )\n",
        "        return result\n",
        "        \n",
        "    def get_most_continuous_morpheme_data(self, morpheme_list, target_morpheme, target_data):\n",
        "        cnt = 0\n",
        "        result_word = {}\n",
        "        tmp_word = \"\"\n",
        "        for  morpheme in morpheme_list:\n",
        "            if morpheme[target_morpheme] == target_data:\n",
        "                cnt += 1\n",
        "                tmp_word += morpheme[\"surface\"]\n",
        "            else:\n",
        "                if cnt <= 0:\n",
        "                    continue\n",
        "                if cnt in result_word:\n",
        "                    result_word[cnt].append(tmp_word)\n",
        "                else:\n",
        "                    result_word[cnt] = [tmp_word]\n",
        "                cnt = 0\n",
        "                tmp_word = \"\"\n",
        "        return result_word[max(result_word.keys())]\n",
        "\n",
        "    def mapping_word_flequency(self, morpheme_list):\n",
        "        return collections.Counter([x[\"surface\"] for x in morpheme_list])\n",
        "\n",
        "\n",
        "class GraphCreator:\n",
        "\n",
        "    def create_bar_graph(self, data):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.bar(np.arange(1, len(data) + 1), [y[1] for y in data], tick_label=[x[0] for x in data])\n",
        "        return plt\n",
        "\n",
        "    def create_histogram(self, data):\n",
        "        fig, ax = plt.subplots()\n",
        "        plt.xlim(xmin=1, xmax=20)\n",
        "        n, bins, patches = ax.hist([y[1] for y in data], bins=20, range=(1,20))\n",
        "        return plt\n",
        "\n",
        "\n",
        "analyzer = MeCabAnalyzer()\n",
        "with codecs.open(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.mecab\", \"r\", \"utf-8\") as f:\n",
        "    node = analyzer.parser(f.readline())\n",
        "    result = []\n",
        "    while node:\n",
        "        if len(node.split(\"\\n\")) <= 3:\n",
        "            break\n",
        "        result.append(analyzer.morpheme_to_dict(node))\n",
        "        node = analyzer.parser(f.readline())\n",
        "\n",
        "most_continuous_words = analyzer.mapping_word_flequency(result)\n",
        "graph_creator = GraphCreator()\n",
        "graph_creator.create_histogram(most_continuous_words.most_common()).show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}