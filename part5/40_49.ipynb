{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "40_49.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhXYUig2tU-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8SjGhfCsYMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mecab\n",
        "!apt update && apt install -y \\\n",
        "    curl \\\n",
        "    file \\\n",
        "    git \\\n",
        "    libmecab-dev \\\n",
        "    make \\\n",
        "    mecab \\\n",
        "    mecab-ipadic-utf8 \\\n",
        "    swig \\\n",
        "    xz-utils \n",
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Eq7QG7Bsm_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CRF++\n",
        "import os\n",
        "\n",
        "filename_crfpp = 'crfpp.tar.gz'\n",
        "!wget \"https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7QVR6VXJ5dWExSTQ\" \\\n",
        "    -O $filename_crfpp\n",
        "!tar zxvf $filename_crfpp\n",
        "!cd CRF++-0.58 && ./configure && make && make install\n",
        "\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/lib' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_xDpxXGsqZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CaboCha\n",
        "# https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7SDd1Q1dUQkZQaUU よりダウンロード\n",
        "filename_cabocha = 'cabocha-0.69.tar.bz2'\n",
        "#!wget \"$url_cabocha\" -O $filename_cabocha\n",
        "!tar -jxf $filename_cabocha\n",
        "!cd cabocha-0.69 && ./configure --with-mecab-config=`which mecab-config` --with-charset=UTF8\n",
        "!cd cabocha-0.69 && make && make check && make install\n",
        "!cabocha --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPInH8OYvYLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cabocha with python\n",
        "!cd cabocha-0.69/python && python setup.py build_ext && python setup.py install\n",
        "!cd cabocha-0.69/python && ldconfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heGVRRr3YUGj",
        "colab_type": "text"
      },
      "source": [
        "# 第5章: 係り受け解析\n",
        "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
        "\n",
        "## 40. 係り受け解析結果の読み込み（形態素）\n",
        "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqEvzweKu83Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cabocha -f1 \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt\" -o \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV1xMu0ZiGGP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0NhgkGGd8Ov",
        "colab_type": "code",
        "outputId": "72eab7ee-36ce-4c91-e683-b4fde4962b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "class Morph(object):\n",
        "\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "\n",
        "def generate_morph(cabocha_file):\n",
        "    parser = CaboCha.Parser()\n",
        "    morph_data = []\n",
        "    with codecs.open(cabocha_file, \"r\", \"utf-8\") as in_f:\n",
        "        sentence = []\n",
        "        for line in in_f.readlines():\n",
        "            words = line.split(' ')\n",
        "            if words[0] == \"*\":\n",
        "                continue\n",
        "            elif words[0].strip() == \"EOS\":\n",
        "                if len(sentence) > 0:\n",
        "                    morph_data.append(sentence)\n",
        "                    sentence = []\n",
        "                continue\n",
        "            all_morph = words[0].split(\",\")\n",
        "            morph = Morph(all_morph[0].split(\"\\t\")[0], all_morph[6], all_morph[0].split(\"\\t\")[1], all_morph[1])\n",
        "            sentence.append(morph)\n",
        "    return morph_data\n",
        "\n",
        "morph_result = generate_morph(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\")\n",
        "for m in morph_result[2]:\n",
        "    print(m.__dict__.values())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_values(['名前', '名前', '名詞', '一般'])\n",
            "dict_values(['は', 'は', '助詞', '係助詞'])\n",
            "dict_values(['まだ', 'まだ', '副詞', '助詞類接続'])\n",
            "dict_values(['無い', '無い', '形容詞', '自立'])\n",
            "dict_values(['。', '。', '記号', '句点'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HazrRkc7cSiy",
        "colab_type": "text"
      },
      "source": [
        "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
        "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNrQzzAfVn-j",
        "colab_type": "code",
        "outputId": "811ae756-4ba5-4479-d5a7-3d4ec54f76ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "\n",
        "    戻り値：\n",
        "    1文のChunkクラスのリスト\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "        raise StopIteration\n",
        "\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for i, chunks in enumerate(generate_chunk(neko_file), 1):\n",
        "    # 8文目を表示\n",
        "    if i == 8:\n",
        "        for j, chunk in enumerate(chunks):\n",
        "            print('[{}]{}'.format(j, chunk))\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]吾輩は\tsrcs[]\tdst[5]\n",
            "[1]ここで\tsrcs[]\tdst[2]\n",
            "[2]始めて\tsrcs[1]\tdst[3]\n",
            "[3]人間という\tsrcs[2]\tdst[4]\n",
            "[4]ものを\tsrcs[3]\tdst[5]\n",
            "[5]見た。\tsrcs[0, 4]\tdst[-1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOOUt-OTZ6t0",
        "colab_type": "text"
      },
      "source": [
        "## 42. 係り元と係り先の文節の表示\n",
        "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONayFJ5C4OoA",
        "colab_type": "code",
        "outputId": "dc4ed076-b51d-4524-e95c-1e46163f37f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def generate_related_cabocha(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        chunk_list = []\n",
        "        for c in chunk_data:\n",
        "            surface = \"\"\n",
        "            for morph in c.morphs:\n",
        "                surface += morph.surface\n",
        "            obj = {surface: {\"dst\": c.dst, \"srcs\": c.srcs}}\n",
        "            chunk_list.append(obj)\n",
        "        for clause in chunk_list:\n",
        "            srcs = clause[list(clause.keys())[0]][\"srcs\"]\n",
        "            if not len(srcs):\n",
        "                continue\n",
        "            for chunk_ind in srcs:\n",
        "                src_word = re.sub(r\"[。、]\", \"\", list(chunk_list[chunk_ind].keys())[0])\n",
        "                effected_word = re.sub(r\"[。、]\", \"\", list(clause.keys())[0])\n",
        "                result.append(\"{}\\t{}\".format(src_word, effected_word))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for cnt, word in enumerate(generate_related_cabocha(neko_file)):\n",
        "    # 量が多いので5行分だけ出力\n",
        "    if cnt > 5:\n",
        "        break\n",
        "    print(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "　\t猫である\n",
            "吾輩は\t猫である\n",
            "名前は\t無い\n",
            "まだ\t無い\n",
            "　どこで\t生れたか\n",
            "生れたか\tつかぬ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFVQR7mCtPC2",
        "colab_type": "text"
      },
      "source": [
        "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
        "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ． "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axjDXijiyGwA",
        "colab_type": "code",
        "outputId": "9473dbca-2bd9-4da6-b7c4-bb09239ba39a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def delete_panctuation(word):\n",
        "    return re.sub(r\"[、。]\", \"\", word)\n",
        "\n",
        "def generate_related_cabocha(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        chunk_list = []\n",
        "        for c in chunk_data:\n",
        "            surface = \"\"\n",
        "            pos_list = []\n",
        "            for morph in c.morphs:\n",
        "                surface += morph.surface\n",
        "                pos_list.append(morph.pos)\n",
        "            obj = {surface: {\"dst\": c.dst, \"srcs\": c.srcs, \"pos\": pos_list}}\n",
        "            chunk_list.append(obj)\n",
        "        for clause in chunk_list:\n",
        "            srcs = clause[list(clause.keys())[0]][\"srcs\"]\n",
        "            pos = clause[list(clause.keys())[0]][\"pos\"]\n",
        "            if not len(srcs):\n",
        "                continue\n",
        "            for chunk_ind in srcs:\n",
        "                src_word = list(chunk_list[chunk_ind].keys())[0]\n",
        "                effected_word = list(clause.keys())[0]\n",
        "                if \"名詞\" in chunk_list[chunk_ind][src_word][\"pos\"] and \"動詞\" in pos:\n",
        "                    # src:名詞 -> eff:動詞\n",
        "                    result.append(\"{}\\t{}\".format(\n",
        "                        delete_panctuation(src_word), delete_panctuation(effected_word)))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for cnt, word in enumerate(generate_related_cabocha(neko_file)):\n",
        "    # 量が多いので5行分だけ出力\n",
        "    if cnt > 5:\n",
        "        break\n",
        "    print(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "　どこで\t生れたか\n",
            "見当が\tつかぬ\n",
            "所で\t泣いて\n",
            "ニャーニャー\t泣いて\n",
            "いた事だけは\t記憶している\n",
            "ここで\t始めて\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--VFPoBdx7_S",
        "colab_type": "text"
      },
      "source": [
        "## 44. 係り受け木の可視化\n",
        "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Cf16tFzcfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install graphviz pydot pillow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oog55LL1Hmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "from pydot import Dot, Node, Edge\n",
        "from PIL import Image\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "\n",
        "    戻り値：\n",
        "    1文のChunkクラスのリスト\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def create_graph(chunk_data):\n",
        "    graph = Dot(graph_type='graph')\n",
        "    nodes = []\n",
        "    for i, chunk in enumerate(chunk_data):\n",
        "        surface = \"\"\n",
        "        for w in chunk.morphs:\n",
        "            surface+=w.surface\n",
        "        node = Node(f'\"{i}\"', label=surface)\n",
        "        nodes.append(node)\n",
        "        graph.add_node(node)\n",
        "    for i, chunk in enumerate(chunk_data):\n",
        "        if chunk.dst == -1:\n",
        "            continue\n",
        "        node_src = nodes[i]\n",
        "        node_dst = nodes[chunk.dst]\n",
        "        edge = Edge(node_src, node_dst)\n",
        "        graph.add_edge(edge)\n",
        "    return graph\n",
        "\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for i, chunks in enumerate(generate_chunk(neko_file), 1):\n",
        "    # 8文目を表示\n",
        "    if i == 8:\n",
        "        graph = create_graph(chunks)\n",
        "        graph.write_png('q44.png')\n",
        "        Image.open('q44.png').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNmIotU5Aa8E",
        "colab_type": "text"
      },
      "source": [
        "## 45. 動詞の格パターンの抽出\n",
        "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
        "\n",
        "\n",
        "*   動詞を含む文節において，最左の動詞の基本形を述語とする\n",
        "*   述語に係る助詞を格とする\n",
        "*   述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
        "\n",
        "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
        "\n",
        "```\n",
        "始める  で\n",
        "見る    は を\n",
        "```\n",
        "\n",
        "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
        "\n",
        "\n",
        "\n",
        "*   コーパス中で頻出する述語と格パターンの組み合わせ\n",
        "*   「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFVk71Ne11Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 要件\n",
        "# 述語の格を知りたい\n",
        "# 動詞＝述語と動詞に係っている文節の助詞を格\n",
        "# 出力→述語 \\t 格\n",
        "# 述語：動詞を含む文節における動詞の基本形(surface)\n",
        "# 格：述語に係る助詞\n",
        "# 　　※助詞（文節）が複数ある場合、すべての助詞をスペース区切りで辞書順に並べる\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXI2v6m-qWTG",
        "colab_type": "code",
        "outputId": "578c92a1-c5d6-46ed-c43a-5848208e6057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def delete_panctuation(word):\n",
        "    return re.sub(r\"[、。]\", \"\", word)\n",
        "\n",
        "def generate_verb_cases(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        chunk_list = []\n",
        "        for c in chunk_data:\n",
        "            surface = \"\"\n",
        "            pos_list = []\n",
        "            for morph in c.morphs:\n",
        "                surface += morph.surface\n",
        "                pos_list.append(morph.pos)\n",
        "            obj = {surface: {\"dst\": c.dst, \"srcs\": c.srcs, \"pos\": pos_list, \"morphs\": c.morphs}}\n",
        "            chunk_list.append(obj)\n",
        "        for clause in chunk_list:\n",
        "            pos = clause[list(clause.keys())[0]][\"pos\"]\n",
        "            srcs = clause[list(clause.keys())[0]][\"srcs\"]\n",
        "            morphs = clause[list(clause.keys())[0]][\"morphs\"]\n",
        "            src_words = []\n",
        "            if \"動詞\" not in pos:\n",
        "                continue\n",
        "            for m_data in morphs:\n",
        "                if m_data.pos == \"動詞\":\n",
        "                    verb = m_data.base\n",
        "                    break\n",
        "                \n",
        "            for chunk_ind in srcs:\n",
        "                for s in chunk_list[chunk_ind][list(chunk_list[chunk_ind].keys())[0]][\"morphs\"]:\n",
        "                    if s.pos != \"助詞\":\n",
        "                        continue\n",
        "                    src_words.append(s.surface)\n",
        "            result.append(\"{}\\t{}\".format(\n",
        "                delete_panctuation(verb), \n",
        "                delete_panctuation(\" \".join(src_words))\n",
        "            ))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "verb_file = \"verb_list.txt\"\n",
        "for ind, line in enumerate(generate_verb_cases(neko_file)):\n",
        "    if ind <=6:\n",
        "        print(line)\n",
        "    with codecs.open(verb_file, \"a\", \"utf-8\") as f:\n",
        "        f.write(line + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "生れる\tで\n",
            "つく\tか が\n",
            "する\t\n",
            "泣く\tで\n",
            "する\tて だけ は\n",
            "始める\tで\n",
            "見る\tは を\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRV20ht_5I36",
        "colab_type": "code",
        "outputId": "717067ac-6542-447e-83af-3760b908ab1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# コーパス中で頻出する述語と格パターンの組み合わせ\n",
        "! sort verb_list.txt | uniq -c | sort -nr | head -1\n",
        "# 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n",
        "! grep \"する\" verb_list.txt | uniq -c | sort -nr | head -1\n",
        "! grep \"見る\" verb_list.txt | uniq -c | sort -nr | head -1\n",
        "! grep \"与える\" verb_list.txt | uniq -c | sort -nr | head -1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   2413 ある\tが\n",
            "      7 する\tで\n",
            "      8 見る\tを\n",
            "      2 与える\tに を\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLESW91N85Jo",
        "colab_type": "text"
      },
      "source": [
        "## 46. 動詞の格フレーム情報の抽出\n",
        "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
        "\n",
        "*   項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
        "*   述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
        "\n",
        "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
        "\n",
        "```\n",
        "始める  で      ここで\n",
        "見る    は を   吾輩は ものを\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDr-vYx484v1",
        "colab_type": "code",
        "outputId": "0235b730-6d34-46d4-c7b9-5a1ea581492e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def delete_panctuation(word):\n",
        "    return re.sub(r\"[、。]\", \"\", word)\n",
        "\n",
        "def get_src_words(chunk_morph, srcs):\n",
        "    result = []\n",
        "    for ind, morph in enumerate(chunk_morph):\n",
        "        if ind in srcs:\n",
        "            result.append(list(morph.keys())[0])\n",
        "    return result\n",
        "\n",
        "def generate_verb_cases(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        chunk_list = []\n",
        "        for c in chunk_data:\n",
        "            surface = \"\"\n",
        "            pos_list = []\n",
        "            for morph in c.morphs:\n",
        "                surface += morph.surface\n",
        "                pos_list.append(morph.pos)\n",
        "            obj = {surface: {\"dst\": c.dst, \"srcs\": c.srcs, \"pos\": pos_list, \"morphs\": c.morphs}}\n",
        "            chunk_list.append(obj)\n",
        "        for clause in chunk_list:\n",
        "            pos = clause[list(clause.keys())[0]][\"pos\"]\n",
        "            srcs = clause[list(clause.keys())[0]][\"srcs\"]\n",
        "            morphs = clause[list(clause.keys())[0]][\"morphs\"]\n",
        "            target_src_words = []\n",
        "            src_words = get_src_words(chunk_list, srcs)\n",
        "            if \"動詞\" not in pos:\n",
        "                continue\n",
        "            for m_data in morphs:\n",
        "                if m_data.pos == \"動詞\":\n",
        "                    verb = m_data.base\n",
        "                    break\n",
        "                \n",
        "            for chunk_ind in srcs:\n",
        "                for s in chunk_list[chunk_ind][list(chunk_list[chunk_ind].keys())[0]][\"morphs\"]:\n",
        "                    if s.pos != \"助詞\":\n",
        "                        continue\n",
        "                    target_src_words.append(s.surface)\n",
        "            result.append(\"{}\\t{}\\t{}\".format(\n",
        "                delete_panctuation(verb), \n",
        "                delete_panctuation(\" \".join(target_src_words)),\n",
        "                \" \".join(src_words)\n",
        "            ))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for ind, line in enumerate(generate_verb_cases(neko_file)):\n",
        "    if ind <=6:\n",
        "        print(line)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "生れる\tで\t　どこで\n",
            "つく\tか が\t生れたか とんと 見当が\n",
            "する\t\t\n",
            "泣く\tで\t所で ニャーニャー\n",
            "する\tて だけ は\t泣いて いた事だけは\n",
            "始める\tで\tここで\n",
            "見る\tは を\t吾輩は ものを\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siU9xv2mrlah",
        "colab_type": "text"
      },
      "source": [
        "## 47. 機能動詞構文のマイニング\n",
        "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
        "\n",
        "*   「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
        "*   述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
        "*   述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
        "*   述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
        "\n",
        "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
        "\n",
        "```\n",
        "返事をする      と に は        及ばんさと 手紙に 主人は\n",
        "```\n",
        "\n",
        "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
        "\n",
        "*   コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
        "*   コーパス中で頻出する述語と助詞パターン"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO4NvFQKU8VF",
        "colab_type": "code",
        "outputId": "e9a67034-cfaa-4bf3-b764-57366ba07dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def delete_panctuation(word):\n",
        "    return re.sub(r\"[、。]\", \"\", word)\n",
        "\n",
        "def get_src_words(chunk_morph, srcs):\n",
        "    result = []\n",
        "    for ind, morph in enumerate(chunk_morph):\n",
        "        key_name = list(morph.keys())[0]\n",
        "        if ind in srcs:\n",
        "            result.append(key_name)\n",
        "    return result\n",
        "\n",
        "def is_sahen_setuzoku_noun(morph):\n",
        "    for i, m in enumerate(morph):\n",
        "        if m.pos == \"名詞\" and m.pos1 == \"サ変接続\":\n",
        "            if len(morph) <= i + 1:\n",
        "                return False\n",
        "            if morph[i + 1].pos == \"助詞\" and morph[i + 1].base == \"を\":\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def get_sahen_setuzoku_noun(morphs):\n",
        "    for i, m in enumerate(morphs):\n",
        "        if m.pos == \"名詞\" and m.pos1 == \"サ変接続\":\n",
        "            if len(morphs) <= i + 1:\n",
        "                return None\n",
        "            if morphs[i + 1].pos == \"助詞\" and morphs[i + 1].base == \"を\":\n",
        "                return m.surface + morphs[i + 1].surface \n",
        "    return None\n",
        "\n",
        "def create_morph_obj(chunk):\n",
        "     surface = \"\"\n",
        "     pos_list = []\n",
        "     for morph in chunk.morphs:\n",
        "         surface += morph.surface\n",
        "         pos_list.append(morph.pos)\n",
        "     return {surface: {\n",
        "         \"dst\": chunk.dst, \"srcs\": chunk.srcs, \"pos\": pos_list, \"morphs\": chunk.morphs}}\n",
        "\n",
        "def is_base_target_pos_data(pos):\n",
        "    return \"動詞\" in pos\n",
        "\n",
        "def get_base_verb(morphs):\n",
        "    for m_data in morphs:\n",
        "        if m_data.pos == \"動詞\":\n",
        "            return m_data.base\n",
        "\n",
        "def add_sahen_word_to_verb(verb, src_target_morphs):\n",
        "    if not is_sahen_setuzoku_noun(src_target_morphs):\n",
        "        # 係り元がサ変接続名詞でなければSKIP\n",
        "        return None\n",
        "    sahen_word = get_sahen_setuzoku_noun(src_target_morphs)\n",
        "    if not sahen_word:\n",
        "        return None\n",
        "    return sahen_word + verb\n",
        "\n",
        "def generate_verb_cases(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        chunk_list = []\n",
        "        for c in chunk_data:\n",
        "            obj = create_morph_obj(c)\n",
        "            chunk_list.append(obj)\n",
        "        for clause in chunk_list:\n",
        "            # clause = {surface: {\"dst\": c.dst, \"srcs\": c.srcs, \"pos\": pos_list, \"morphs\": c.morphs}}\n",
        "            pos = clause[list(clause.keys())[0]][\"pos\"]\n",
        "            srcs = clause[list(clause.keys())[0]][\"srcs\"]\n",
        "            morphs = clause[list(clause.keys())[0]][\"morphs\"]\n",
        "            src_clauses = get_src_words(chunk_list, srcs)\n",
        "            if not is_base_target_pos_data(pos):\n",
        "                # 動詞を対象とするので、動詞以外はSKIP\n",
        "                continue\n",
        "            verb = get_base_verb(morphs)\n",
        "                \n",
        "            target_src_words = []\n",
        "            for chunk_ind in srcs:\n",
        "                src_target_morphs = chunk_list[chunk_ind][list(chunk_list[chunk_ind].keys())[0]][\"morphs\"]\n",
        "                for m in src_target_morphs:\n",
        "                    if m.pos != \"助詞\":\n",
        "                        continue\n",
        "                    target_src_words.append(m.surface)\n",
        "                verb_added_sahen = add_sahen_word_to_verb(verb, src_target_morphs)\n",
        "            if verb_added_sahen and target_src_words:\n",
        "                result.append(\"{}\\t{}\\t{}\".format(\n",
        "                    verb_added_sahen, \n",
        "                    delete_panctuation(\" \".join(target_src_words)),\n",
        "                    \" \".join(src_clauses)\n",
        "                ))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for ind, line in enumerate(generate_verb_cases(neko_file)):\n",
        "    with codecs.open('q_47.txt', \"a\", \"utf-8\") as f:\n",
        "        f.write(line + \"\\n\")\n",
        "    if \"及ばんさと\" in line:\n",
        "        print(line)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "返事をする\tさ と は に を\t及ばんさと、 主人は 手紙に 返事を\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2jmxy5n6Ffh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
        "コーパス中で頻出する述語と助詞パターン"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHTvAzf46Tbm",
        "colab_type": "code",
        "outputId": "b7ee3bb0-d9f8-480c-990a-75a358b7e3c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
        "! cut -f 1 q_47.txt | sort | uniq -c | sort -nr | head -1\n",
        "\n",
        "! cut -f 2,3 q_47.txt # | sort | uniq -c | sort -nr | head -1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     30 返事をする\n",
            "      7 を\t運動を\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAuX0CQ98yk6",
        "colab_type": "text"
      },
      "source": [
        "## 48. 名詞から根へのパスの抽出\n",
        "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
        "\n",
        "*   各文節は（表層形の）形態素列で表現する\n",
        "*   パスの開始文節から終了文節に至るまで，各文節の表現を\"->\"で連結する\n",
        "\n",
        "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
        "\n",
        "```\n",
        "吾輩は -> 見た\n",
        "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
        "人間という -> ものを -> 見た\n",
        "ものを -> 見た\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz84kP3sQ3Al",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "56cb409e-6c28-4e8f-dc3e-7c6c3f4ba1e8"
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(0) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def delete_panctuation(word):\n",
        "    return re.sub(r\"[、。]\", \"\", word)\n",
        "\n",
        "def get_src_words(chunk_morph, srcs):\n",
        "    result = []\n",
        "    for ind, morph in enumerate(chunk_morph):\n",
        "        key_name = list(morph.keys())[0]\n",
        "        if ind in srcs:\n",
        "            result.append(key_name)\n",
        "    return result\n",
        "\n",
        "def is_sahen_setuzoku_noun(morph):\n",
        "    for i, m in enumerate(morph):\n",
        "        if m.pos == \"名詞\" and m.pos1 == \"サ変接続\":\n",
        "            if len(morph) <= i + 1:\n",
        "                return False\n",
        "            if morph[i + 1].pos == \"助詞\" and morph[i + 1].base == \"を\":\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def get_sahen_setuzoku_noun(morphs):\n",
        "    for i, m in enumerate(morphs):\n",
        "        if m.pos == \"名詞\" and m.pos1 == \"サ変接続\":\n",
        "            if len(morphs) <= i + 1:\n",
        "                return None\n",
        "            if morphs[i + 1].pos == \"助詞\" and morphs[i + 1].base == \"を\":\n",
        "                return m.surface + morphs[i + 1].surface \n",
        "    return None\n",
        "\n",
        "def create_morph_obj(chunk):\n",
        "     surface = \"\"\n",
        "     pos_list = []\n",
        "     for morph in chunk.morphs:\n",
        "         surface += morph.surface\n",
        "         pos_list.append(morph.pos)\n",
        "     return {surface: {\n",
        "        # \"dst\": chunk.dst, \"srcs\": chunk.srcs, \"pos\": pos_list, \"morphs\": chunk.morphs}}\n",
        "         \"dst\": chunk.dst, \"srcs\": chunk.srcs, \"pos\": pos_list}}\n",
        "\n",
        "def is_base_target_pos_data(pos, target_pos):\n",
        "    return target_pos in pos\n",
        "\n",
        "def get_base_word(morphs, base_pos_word):\n",
        "    for m_data in morphs:\n",
        "        if m_data.pos == base_pos_word:\n",
        "            return m_data.base\n",
        "\n",
        "def add_sahen_word_to_verb(verb, src_target_morphs):\n",
        "    if not is_sahen_setuzoku_noun(src_target_morphs):\n",
        "        # 係り元がサ変接続名詞でなければSKIP\n",
        "        return None\n",
        "    sahen_word = get_sahen_setuzoku_noun(src_target_morphs)\n",
        "    if not sahen_word:\n",
        "        return None\n",
        "    return sahen_word + verb\n",
        "\n",
        "def create_tree_src_data(line_data):\n",
        "    line_tree_data = []\n",
        "    for clause in line_data:\n",
        "        clause_word = list(clause.keys())[0]\n",
        "        pos = clause[clause_word][\"pos\"]\n",
        "        dst = clause[clause_word][\"dst\"]\n",
        "        line_tree_data.append({\"surface\": clause_word, \"pos\": pos, \"dst\": dst})\n",
        "    return line_tree_data\n",
        "\n",
        "def create_line_tree(line_tree_data):\n",
        "    tree = []\n",
        "    for line in line_tree_data:\n",
        "        if \"名詞\" not in line[\"pos\"]:\n",
        "            continue\n",
        "        dst = line[\"dst\"]\n",
        "        tmp_tree = [line[\"surface\"]]\n",
        "        while dst != -1:\n",
        "            tmp_tree.append(line_tree_data[dst][\"surface\"])\n",
        "            dst = line_tree_data[dst][\"dst\"]\n",
        "        tree.append(tmp_tree)\n",
        "    return tree\n",
        "\n",
        "\n",
        "def create_tree_list(line_data):\n",
        "    line_tree_data = create_tree_src_data(line_data)\n",
        "    return create_line_tree(line_tree_data)\n",
        "        \n",
        "def generate_clause_tree(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        if ind != 7:\n",
        "            continue\n",
        "        chunk_list = [create_morph_obj(c) for c in chunk_data]\n",
        "        # 1文分\n",
        "        for tree_data in create_tree_list(chunk_list):\n",
        "            result.append(\" -> \".join(tree_data))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for ind, line in enumerate(generate_clause_tree(neko_file)):\n",
        "    print(line)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "吾輩は -> 見た。\n",
            "ここで -> 始めて -> 人間という -> ものを -> 見た。\n",
            "人間という -> ものを -> 見た。\n",
            "ものを -> 見た。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJYX9lTMZw41",
        "colab_type": "text"
      },
      "source": [
        "## 49. 名詞間の係り受けパスの抽出\n",
        "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
        "\n",
        "*   問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"->\"で連結して表現する\n",
        "*   文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
        "\n",
        "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
        "\n",
        "*    文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
        "*     上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を\"|\"で連結して表示\n",
        "\n",
        "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
        "\n",
        "```\n",
        "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
        "Xは | Yという -> ものを | 見た\n",
        "Xは | Yを | 見た\n",
        "Xで -> 始めて -> Y\n",
        "Xで -> 始めて -> 人間という -> Y\n",
        "Xという -> Y\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epaW6y09Zwdt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "03489431-38cc-4dd1-8482-53f49b64b514"
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(0) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def delete_panctuation(word):\n",
        "    return re.sub(r\"[、。]\", \"\", word)\n",
        "\n",
        "def get_src_words(chunk_morph, srcs):\n",
        "    result = []\n",
        "    for ind, morph in enumerate(chunk_morph):\n",
        "        key_name = list(morph.keys())[0]\n",
        "        if ind in srcs:\n",
        "            result.append(key_name)\n",
        "    return result\n",
        "\n",
        "def is_sahen_setuzoku_noun(morph):\n",
        "    for i, m in enumerate(morph):\n",
        "        if m.pos == \"名詞\" and m.pos1 == \"サ変接続\":\n",
        "            if len(morph) <= i + 1:\n",
        "                return False\n",
        "            if morph[i + 1].pos == \"助詞\" and morph[i + 1].base == \"を\":\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def get_sahen_setuzoku_noun(morphs):\n",
        "    for i, m in enumerate(morphs):\n",
        "        if m.pos == \"名詞\" and m.pos1 == \"サ変接続\":\n",
        "            if len(morphs) <= i + 1:\n",
        "                return None\n",
        "            if morphs[i + 1].pos == \"助詞\" and morphs[i + 1].base == \"を\":\n",
        "                return m.surface + morphs[i + 1].surface \n",
        "    return None\n",
        "\n",
        "def create_morph_obj(chunk):\n",
        "     surface = \"\"\n",
        "     pos_list = []\n",
        "     for morph in chunk.morphs:\n",
        "         surface += morph.surface\n",
        "         pos_list.append(morph.pos)\n",
        "     return {surface: {\n",
        "         \"dst\": chunk.dst, \"srcs\": chunk.srcs, \"pos\": pos_list, \"morphs\": chunk.morphs}}\n",
        "        # \"dst\": chunk.dst, \"srcs\": chunk.srcs, \"pos\": pos_list}}\n",
        "\n",
        "def is_base_target_pos_data(pos, target_pos):\n",
        "    return target_pos in pos\n",
        "\n",
        "def get_base_word(morphs, base_pos_word):\n",
        "    for m_data in morphs:\n",
        "        if m_data.pos == base_pos_word:\n",
        "            return m_data.base\n",
        "\n",
        "def add_sahen_word_to_verb(verb, src_target_morphs):\n",
        "    if not is_sahen_setuzoku_noun(src_target_morphs):\n",
        "        # 係り元がサ変接続名詞でなければSKIP\n",
        "        return None\n",
        "    sahen_word = get_sahen_setuzoku_noun(src_target_morphs)\n",
        "    if not sahen_word:\n",
        "        return None\n",
        "    return sahen_word + verb\n",
        "\n",
        "def create_tree_src_data(line_data):\n",
        "    line_tree_data = []\n",
        "    for clause in line_data:\n",
        "        clause_word = list(clause.keys())[0]\n",
        "        pos = clause[clause_word][\"pos\"]\n",
        "        dst = clause[clause_word][\"dst\"]\n",
        "        morphs = clause[clause_word][\"morphs\"]\n",
        "        line_tree_data.append({\n",
        "            \"surface\": clause_word, \"pos\": pos, \"dst\": dst, \"morphs\": morphs})\n",
        "    return line_tree_data\n",
        "\n",
        "def create_noun_data_list(line_tree_data):\n",
        "    noun_data_list = []\n",
        "    for line in line_tree_data:\n",
        "        if \"名詞\" not in line[\"pos\"]:\n",
        "            continue\n",
        "        for m in line[\"morphs\"]:\n",
        "            if m.pos == \"名詞\":\n",
        "                noun_word = m.surface\n",
        "                break\n",
        "        noun_data_list.append({\"clause\": line[\"surface\"], \"word\": noun_word})\n",
        "    return noun_data_list\n",
        "\n",
        "def create_line_tree(line_tree_data):\n",
        "    tree = []\n",
        "    for line in line_tree_data:\n",
        "        if \"名詞\" not in line[\"pos\"]:\n",
        "            continue\n",
        "        dst = line[\"dst\"]\n",
        "        tmp_tree = [line[\"surface\"]]\n",
        "        while dst != -1:\n",
        "            tmp_tree.append(line_tree_data[dst][\"surface\"])\n",
        "            dst = line_tree_data[dst][\"dst\"]\n",
        "        tree.append(tmp_tree)\n",
        "    return tree\n",
        "\n",
        "def get_common_node_noun_releation_tree(common_node_tree, all_noun_pair):\n",
        "    tree = []\n",
        "    # 3. x, yは一度も出会わずに同じ目的地点で合流する\n",
        "    for noun_pair in all_noun_pair:\n",
        "        x_tree = []\n",
        "        y_tree = []\n",
        "        for t in common_node_tree:\n",
        "            if t[0] == noun_pair[0][\"clause\"]:\n",
        "                x_tree = t\n",
        "            elif t[0] == noun_pair[1][\"clause\"]:\n",
        "                y_tree = t\n",
        "        if not x_tree or not y_tree:\n",
        "            continue\n",
        "        if delete_panctuation(x_tree[-1]) != delete_panctuation(y_tree[-1]):\n",
        "            # d. xとyの最終地点が一致しない\n",
        "            continue\n",
        "        if noun_pair[0][\"clause\"] in y_tree or noun_pair[1][\"clause\"] in x_tree:\n",
        "            # 同一経路上に存在する場合は別途抽出しているのでここでは除外\n",
        "            continue\n",
        "        if len(x_tree) == 2 and len(y_tree) == 2:\n",
        "            # c. xもyも最終地点に直接合流する\n",
        "            x = x_tree[0].replace(noun_pair[0][\"word\"], \"X\")\n",
        "            y = y_tree[0].replace(noun_pair[1][\"word\"], \"Y\")\n",
        "            tree.append(\" | \".join([x, y, delete_panctuation(x_tree[-1])]))\n",
        "        else:\n",
        "            # a. xはいくつかのノードを経て、最終地点で合流し、yは最終地点に直接合流する\n",
        "            # b. xもyもいくつかのノードを経て、最終地点で合流する\n",
        "            x = x_tree[0].replace(noun_pair[0][\"word\"], \"X\")\n",
        "            y = y_tree[0].replace(noun_pair[1][\"word\"], \"Y\")\n",
        "            x_tree_data = [x, \" -> \".join(x_tree[1:-1])]\n",
        "            y_tree_data = [y, \" -> \".join(y_tree[1:-1])]\n",
        "            if len(x_tree) > 2:\n",
        "                x_tree_data.insert(1, \" -> \")\n",
        "            if len(y_tree) > 2:\n",
        "                y_tree_data.insert(1, \" -> \")\n",
        "            tree.append(\n",
        "                \"\".join(x_tree_data) + \" | \" + \\\n",
        "                \"\".join(y_tree_data) + \" | \" + \\\n",
        "                delete_panctuation(x_tree[-1])\n",
        "            )\n",
        "    return tree\n",
        "\n",
        "def create_all_noun_pair(noun_data_list):\n",
        "    # pair(A, B) と pair(B, A)は同じペアとして扱うので2つもいらない\n",
        "    all_noun_pair = []\n",
        "    added_list = []\n",
        "    for pair in list(itertools.product(noun_data_list, repeat=2)):\n",
        "        if pair[0][\"clause\"] == pair[1][\"clause\"]:\n",
        "            continue\n",
        "        added_target = \"{}, {}\".format(pair[0][\"clause\"], pair[1][\"clause\"])\n",
        "        reverse_added_target = \"{}, {}\".format(pair[1][\"clause\"], pair[0][\"clause\"])\n",
        "        if added_target not in added_list and reverse_added_target not in added_list:\n",
        "            added_list.append(added_target)\n",
        "            all_noun_pair.append(pair)\n",
        "    return all_noun_pair\n",
        "\n",
        "def create_noun_relation_tree(line_tree_data, noun_data_list):\n",
        "    noun_tree = []\n",
        "    common_node_tree = []\n",
        "    all_noun_pair = create_all_noun_pair(noun_data_list)\n",
        "    for tree in create_line_tree(line_tree_data):\n",
        "        for noun_pair in all_noun_pair:\n",
        "            if noun_pair[0][\"clause\"] not in tree and noun_pair[1][\"clause\"] not in tree:\n",
        "                # 1. x, yは経路上出会わない →対象外\n",
        "                continue\n",
        "            elif noun_pair[0][\"clause\"] in tree and noun_pair[1][\"clause\"] in tree:\n",
        "                # 2. xから始まりyで終わる\n",
        "                  # a. xとy間に何もない\n",
        "                  # b. xとy間に他のノードがある\n",
        "                if tree[0] == noun_pair[0][\"word\"]:\n",
        "                    x = noun_pair[0][\"word\"]\n",
        "                    y = noun_pair[1][\"word\"]\n",
        "                    x_clause = noun_pair[0][\"clause\"]\n",
        "                    y_clause = noun_pair[1][\"clause\"]\n",
        "                elif tree[0] == noun_pair[1][\"word\"]:\n",
        "                    x = noun_pair[1][\"word\"]\n",
        "                    y = noun_pair[0][\"word\"]\n",
        "                    x_clause = noun_pair[1][\"clause\"]\n",
        "                    y_clause = noun_pair[0][\"clause\"]\n",
        "                elif tree.index(noun_pair[0][\"clause\"]) < tree.index(noun_pair[1][\"clause\"]):\n",
        "                    x = noun_pair[0][\"word\"]\n",
        "                    y = noun_pair[1][\"word\"]\n",
        "                    x_clause = noun_pair[0][\"clause\"]\n",
        "                    y_clause = noun_pair[1][\"clause\"]\n",
        "                elif tree.index(noun_pair[0][\"clause\"]) > tree.index(noun_pair[1][\"clause\"]):\n",
        "                    x = noun_pair[1][\"word\"]\n",
        "                    y = noun_pair[0][\"word\"]\n",
        "                    x_clause = noun_pair[1][\"clause\"]\n",
        "                    y_clause = noun_pair[0][\"clause\"]\n",
        "                base_noun_tree = tree[tree.index(x_clause):tree.index(y_clause)+1]\n",
        "                base_noun_tree[0] = base_noun_tree[0].replace(x, \"X\")\n",
        "                base_noun_tree[-1] = base_noun_tree[-1].replace(y, \"Y\")\n",
        "                noun_tree.append(delete_panctuation(\" -> \".join(base_noun_tree)))\n",
        "            elif noun_pair[0][\"clause\"] in tree or noun_pair[1][\"clause\"] in tree:\n",
        "                # 3. x, yは一度も出会わずに同じ目的地点で合流する\n",
        "                  # a. xはいくつかのノードを経て、最終地点で合流し、yは最終地点に直接合流する\n",
        "                  # b. xもyもいくつかのノードを経て、最終地点で合流する\n",
        "                  # c. xもyも最終地点に直接合流する\n",
        "                  # d. xとyの最終地点が一致しない\n",
        "                if tree not in common_node_tree:\n",
        "                    common_node_tree.append(tree)\n",
        "    for t in get_common_node_noun_releation_tree(common_node_tree, all_noun_pair):\n",
        "        noun_tree.append(t)\n",
        "    #noun_tree.append([t for t in \\\n",
        "    #    get_common_node_noun_releation_tree(common_node_tree, all_noun_pair)][0])\n",
        "    return noun_tree\n",
        "\n",
        "def create_tree_list(line_data):\n",
        "    line_tree_data = create_tree_src_data(line_data)\n",
        "    noun_data_list = create_noun_data_list(line_tree_data)\n",
        "    return list(set(create_noun_relation_tree(line_tree_data, noun_data_list)))\n",
        "        \n",
        "def generate_clause_tree(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        if ind != 7:\n",
        "            continue\n",
        "        chunk_list = [create_morph_obj(c) for c in chunk_data]\n",
        "        # 1文分\n",
        "        for tree_data in create_tree_list(chunk_list):\n",
        "            result.append(tree_data)\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for ind, line in enumerate(generate_clause_tree(neko_file)):\n",
        "    print(line)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xで -> 始めて -> Yという\n",
            "Xで -> 始めて -> 人間という -> Yを\n",
            "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
            "Xは | Yを | 見た\n",
            "Xという -> Yを\n",
            "Xは | Yという -> ものを | 見た\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}