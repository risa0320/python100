{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "40_49.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhXYUig2tU-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8SjGhfCsYMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mecab\n",
        "!apt update && apt install -y \\\n",
        "    curl \\\n",
        "    file \\\n",
        "    git \\\n",
        "    libmecab-dev \\\n",
        "    make \\\n",
        "    mecab \\\n",
        "    mecab-ipadic-utf8 \\\n",
        "    swig \\\n",
        "    xz-utils \n",
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Eq7QG7Bsm_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CRF++\n",
        "import os\n",
        "\n",
        "filename_crfpp = 'crfpp.tar.gz'\n",
        "!wget \"https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7QVR6VXJ5dWExSTQ\" \\\n",
        "    -O $filename_crfpp\n",
        "!tar zxvf $filename_crfpp\n",
        "!cd CRF++-0.58 && ./configure && make && make install\n",
        "\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/lib' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_xDpxXGsqZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CaboCha\n",
        "# https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7SDd1Q1dUQkZQaUU よりダウンロード\n",
        "filename_cabocha = 'cabocha-0.69.tar.bz2'\n",
        "#!wget \"$url_cabocha\" -O $filename_cabocha\n",
        "!tar -jxf $filename_cabocha\n",
        "!cd cabocha-0.69 && ./configure --with-mecab-config=`which mecab-config` --with-charset=UTF8\n",
        "!cd cabocha-0.69 && make && make check && make install\n",
        "!cabocha --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPInH8OYvYLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cabocha with python\n",
        "!cd cabocha-0.69/python && python setup.py build_ext && python setup.py install\n",
        "!cd cabocha-0.69/python && ldconfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heGVRRr3YUGj",
        "colab_type": "text"
      },
      "source": [
        "# 第5章: 係り受け解析\n",
        "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
        "\n",
        "## 40. 係り受け解析結果の読み込み（形態素）\n",
        "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqEvzweKu83Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cabocha -f1 \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt\" -o \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV1xMu0ZiGGP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0NhgkGGd8Ov",
        "colab_type": "code",
        "outputId": "72eab7ee-36ce-4c91-e683-b4fde4962b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "class Morph(object):\n",
        "\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "\n",
        "def generate_morph(cabocha_file):\n",
        "    parser = CaboCha.Parser()\n",
        "    morph_data = []\n",
        "    with codecs.open(cabocha_file, \"r\", \"utf-8\") as in_f:\n",
        "        sentence = []\n",
        "        for line in in_f.readlines():\n",
        "            words = line.split(' ')\n",
        "            if words[0] == \"*\":\n",
        "                continue\n",
        "            elif words[0].strip() == \"EOS\":\n",
        "                if len(sentence) > 0:\n",
        "                    morph_data.append(sentence)\n",
        "                    sentence = []\n",
        "                continue\n",
        "            all_morph = words[0].split(\",\")\n",
        "            morph = Morph(all_morph[0].split(\"\\t\")[0], all_morph[6], all_morph[0].split(\"\\t\")[1], all_morph[1])\n",
        "            sentence.append(morph)\n",
        "    return morph_data\n",
        "\n",
        "morph_result = generate_morph(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\")\n",
        "for m in morph_result[2]:\n",
        "    print(m.__dict__.values())\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_values(['名前', '名前', '名詞', '一般'])\n",
            "dict_values(['は', 'は', '助詞', '係助詞'])\n",
            "dict_values(['まだ', 'まだ', '副詞', '助詞類接続'])\n",
            "dict_values(['無い', '無い', '形容詞', '自立'])\n",
            "dict_values(['。', '。', '記号', '句点'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HazrRkc7cSiy",
        "colab_type": "text"
      },
      "source": [
        "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
        "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNrQzzAfVn-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "811ae756-4ba5-4479-d5a7-3d4ec54f76ec"
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "\n",
        "    戻り値：\n",
        "    1文のChunkクラスのリスト\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "        raise StopIteration\n",
        "\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for i, chunks in enumerate(generate_chunk(neko_file), 1):\n",
        "    # 8文目を表示\n",
        "    if i == 8:\n",
        "        for j, chunk in enumerate(chunks):\n",
        "            print('[{}]{}'.format(j, chunk))\n",
        "        break"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]吾輩は\tsrcs[]\tdst[5]\n",
            "[1]ここで\tsrcs[]\tdst[2]\n",
            "[2]始めて\tsrcs[1]\tdst[3]\n",
            "[3]人間という\tsrcs[2]\tdst[4]\n",
            "[4]ものを\tsrcs[3]\tdst[5]\n",
            "[5]見た。\tsrcs[0, 4]\tdst[-1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOOUt-OTZ6t0",
        "colab_type": "text"
      },
      "source": [
        "## 42. 係り元と係り先の文節の表示\n",
        "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONayFJ5C4OoA",
        "colab_type": "code",
        "outputId": "dc4ed076-b51d-4524-e95c-1e46163f37f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def generate_related_cabocha(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        chunk_list = []\n",
        "        for c in chunk_data:\n",
        "            surface = \"\"\n",
        "            for morph in c.morphs:\n",
        "                surface += morph.surface\n",
        "            obj = {surface: {\"dst\": c.dst, \"srcs\": c.srcs}}\n",
        "            chunk_list.append(obj)\n",
        "        for clause in chunk_list:\n",
        "            srcs = clause[list(clause.keys())[0]][\"srcs\"]\n",
        "            if not len(srcs):\n",
        "                continue\n",
        "            for chunk_ind in srcs:\n",
        "                src_word = re.sub(r\"[。、]\", \"\", list(chunk_list[chunk_ind].keys())[0])\n",
        "                effected_word = re.sub(r\"[。、]\", \"\", list(clause.keys())[0])\n",
        "                result.append(\"{}\\t{}\".format(src_word, effected_word))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for cnt, word in enumerate(generate_related_cabocha(neko_file)):\n",
        "    # 量が多いので5行分だけ出力\n",
        "    if cnt > 5:\n",
        "        break\n",
        "    print(word)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "　\t猫である\n",
            "吾輩は\t猫である\n",
            "名前は\t無い\n",
            "まだ\t無い\n",
            "　どこで\t生れたか\n",
            "生れたか\tつかぬ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFVQR7mCtPC2",
        "colab_type": "text"
      },
      "source": [
        "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
        "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ． "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axjDXijiyGwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9473dbca-2bd9-4da6-b7c4-bb09239ba39a"
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def delete_panctuation(word):\n",
        "    return re.sub(r\"[、。]\", \"\", word)\n",
        "\n",
        "def generate_related_cabocha(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        chunk_list = []\n",
        "        for c in chunk_data:\n",
        "            surface = \"\"\n",
        "            pos_list = []\n",
        "            for morph in c.morphs:\n",
        "                surface += morph.surface\n",
        "                pos_list.append(morph.pos)\n",
        "            obj = {surface: {\"dst\": c.dst, \"srcs\": c.srcs, \"pos\": pos_list}}\n",
        "            chunk_list.append(obj)\n",
        "        for clause in chunk_list:\n",
        "            srcs = clause[list(clause.keys())[0]][\"srcs\"]\n",
        "            pos = clause[list(clause.keys())[0]][\"pos\"]\n",
        "            if not len(srcs):\n",
        "                continue\n",
        "            for chunk_ind in srcs:\n",
        "                src_word = list(chunk_list[chunk_ind].keys())[0]\n",
        "                effected_word = list(clause.keys())[0]\n",
        "                if \"名詞\" in chunk_list[chunk_ind][src_word][\"pos\"] and \"動詞\" in pos:\n",
        "                    # src:名詞 -> eff:動詞\n",
        "                    result.append(\"{}\\t{}\".format(\n",
        "                        delete_panctuation(src_word), delete_panctuation(effected_word)))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for cnt, word in enumerate(generate_related_cabocha(neko_file)):\n",
        "    # 量が多いので5行分だけ出力\n",
        "    if cnt > 5:\n",
        "        break\n",
        "    print(word)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "　どこで\t生れたか\n",
            "見当が\tつかぬ\n",
            "所で\t泣いて\n",
            "ニャーニャー\t泣いて\n",
            "いた事だけは\t記憶している\n",
            "ここで\t始めて\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--VFPoBdx7_S",
        "colab_type": "text"
      },
      "source": [
        "## 44. 係り受け木の可視化\n",
        "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Cf16tFzcfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install graphviz pydot pillow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oog55LL1Hmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "from pydot import Dot, Node, Edge\n",
        "from PIL import Image\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "\n",
        "    戻り値：\n",
        "    1文のChunkクラスのリスト\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def create_graph(chunk_data):\n",
        "    graph = Dot(graph_type='graph')\n",
        "    nodes = []\n",
        "    for i, chunk in enumerate(chunk_data):\n",
        "        surface = \"\"\n",
        "        for w in chunk.morphs:\n",
        "            surface+=w.surface\n",
        "        node = Node(f'\"{i}\"', label=surface)\n",
        "        nodes.append(node)\n",
        "        graph.add_node(node)\n",
        "    for i, chunk in enumerate(chunk_data):\n",
        "        if chunk.dst == -1:\n",
        "            continue\n",
        "        node_src = nodes[i]\n",
        "        node_dst = nodes[chunk.dst]\n",
        "        edge = Edge(node_src, node_dst)\n",
        "        graph.add_edge(edge)\n",
        "    return graph\n",
        "\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for i, chunks in enumerate(generate_chunk(neko_file), 1):\n",
        "    # 8文目を表示\n",
        "    if i == 8:\n",
        "        graph = create_graph(chunks)\n",
        "        graph.write_png('q44.png')\n",
        "        Image.open('q44.png').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNmIotU5Aa8E",
        "colab_type": "text"
      },
      "source": [
        "## 45. 動詞の格パターンの抽出\n",
        "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
        "\n",
        "\n",
        "*   動詞を含む文節において，最左の動詞の基本形を述語とする\n",
        "*   述語に係る助詞を格とする\n",
        "*   述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
        "\n",
        "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
        "\n",
        "```\n",
        "始める  で\n",
        "見る    は を\n",
        "```\n",
        "\n",
        "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
        "\n",
        "\n",
        "\n",
        "*   コーパス中で頻出する述語と格パターンの組み合わせ\n",
        "*   「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFVk71Ne11Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 要件\n",
        "# 述語の格を知りたい\n",
        "# 動詞＝述語と動詞に係っている文節の助詞を格\n",
        "# 出力→述語 \\t 格\n",
        "# 述語：動詞を含む文節における動詞の基本形(surface)\n",
        "# 格：述語に係る助詞\n",
        "# 　　※助詞（文節）が複数ある場合、すべての助詞をスペース区切りで辞書順に並べる\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXI2v6m-qWTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def delete_panctuation(word):\n",
        "    return re.sub(r\"[、。]\", \"\", word)\n",
        "\n",
        "def generate_verb_cases(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        chunk_list = []\n",
        "        for c in chunk_data:\n",
        "            surface = \"\"\n",
        "            pos_list = []\n",
        "            for morph in c.morphs:\n",
        "                surface += morph.surface\n",
        "                pos_list.append(morph.pos)\n",
        "            obj = {surface: {\"dst\": c.dst, \"srcs\": c.srcs, \"pos\": pos_list, \"morphs\": c.morphs}}\n",
        "            chunk_list.append(obj)\n",
        "        for clause in chunk_list:\n",
        "            pos = clause[list(clause.keys())[0]][\"pos\"]\n",
        "            srcs = clause[list(clause.keys())[0]][\"srcs\"]\n",
        "                \n",
        "            for chunk_ind in srcs:\n",
        "                if \"動詞\" not in pos:\n",
        "                    continue\n",
        "                for m_data in clause[list(clause.keys())[0]][\"morphs\"]:\n",
        "                    if m_data.pos == \"動詞\":\n",
        "                        verb = m_data.base\n",
        "                        break\n",
        "                for s in chunk_list[chunk_ind][list(chunk_list[chunk_ind].keys())[0]][\"morphs\"]:\n",
        "                    if s.pos != \"助詞\":\n",
        "                        continue\n",
        "                    result.append(\"{}\\t{}\".format(\n",
        "                        delete_panctuation(verb), delete_panctuation(s.surface)))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "verb_file = \"verb_list.txt\"\n",
        "for line in generate_verb_cases(neko_file):\n",
        "    with codecs.open(verb_file, \"a\", \"utf-8\") as f:\n",
        "        f.write(line + \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRV20ht_5I36",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "63c34aa9-040e-4936-a633-7e2c32ecaeff"
      },
      "source": [
        "# コーパス中で頻出する述語と格パターンの組み合わせ\n",
        "! sort verb_list.txt | uniq -c | sort -nr | head -1\n",
        "# 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n",
        "! grep \"する\" verb_list.txt | uniq -c | sort -nr | head -1\n",
        "! grep \"見る\" verb_list.txt | uniq -c | sort -nr | head -1\n",
        "! grep \"与える\" verb_list.txt | uniq -c | sort -nr | head -1"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  10405 つく\tか\n",
            "     53 する\tで\n",
            "     56 見る\tて\n",
            "     35 与える\tに\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLESW91N85Jo",
        "colab_type": "text"
      },
      "source": [
        "## 46. 動詞の格フレーム情報の抽出\n",
        "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
        "\n",
        "*   項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
        "*   述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
        "\n",
        "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
        "\n",
        "```\n",
        "始める  で      ここで\n",
        "見る    は を   吾輩は ものを\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDr-vYx484v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}