{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "40_49.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhXYUig2tU-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8SjGhfCsYMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mecab\n",
        "!apt update && apt install -y \\\n",
        "    curl \\\n",
        "    file \\\n",
        "    git \\\n",
        "    libmecab-dev \\\n",
        "    make \\\n",
        "    mecab \\\n",
        "    mecab-ipadic-utf8 \\\n",
        "    swig \\\n",
        "    xz-utils \n",
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Eq7QG7Bsm_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CRF++\n",
        "import os\n",
        "\n",
        "filename_crfpp = 'crfpp.tar.gz'\n",
        "!wget \"https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7QVR6VXJ5dWExSTQ\" \\\n",
        "    -O $filename_crfpp\n",
        "!tar zxvf $filename_crfpp\n",
        "!cd CRF++-0.58 && ./configure && make && make install\n",
        "\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/lib' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_xDpxXGsqZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CaboCha\n",
        "# https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7SDd1Q1dUQkZQaUU よりダウンロード\n",
        "filename_cabocha = 'cabocha-0.69.tar.bz2'\n",
        "#!wget \"$url_cabocha\" -O $filename_cabocha\n",
        "!tar -jxf $filename_cabocha\n",
        "!cd cabocha-0.69 && ./configure --with-mecab-config=`which mecab-config` --with-charset=UTF8\n",
        "!cd cabocha-0.69 && make && make check && make install\n",
        "!cabocha --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPInH8OYvYLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cabocha with python\n",
        "!cd cabocha-0.69/python && python setup.py build_ext && python setup.py install\n",
        "!cd cabocha-0.69/python && ldconfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heGVRRr3YUGj",
        "colab_type": "text"
      },
      "source": [
        "# 第5章: 係り受け解析\n",
        "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
        "\n",
        "## 40. 係り受け解析結果の読み込み（形態素）\n",
        "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqEvzweKu83Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cabocha -f1 \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt\" -o \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV1xMu0ZiGGP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0NhgkGGd8Ov",
        "colab_type": "code",
        "outputId": "4f8a1b22-bb2b-4d82-95cd-17fb6897277b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "class Morph(object):\n",
        "\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "\n",
        "def generate_morph(cabocha_file):\n",
        "    parser = CaboCha.Parser()\n",
        "    morph_data = []\n",
        "    with codecs.open(cabocha_file, \"r\", \"utf-8\") as in_f:\n",
        "        sentence = []\n",
        "        for line in in_f.readlines():\n",
        "            words = line.split(' ')\n",
        "            if words[0] == \"*\":\n",
        "                continue\n",
        "            elif words[0].strip() == \"EOS\":\n",
        "                if len(sentence) > 0:\n",
        "                    morph_data.append(sentence)\n",
        "                    sentence = []\n",
        "                continue\n",
        "            all_morph = words[0].split(\",\")\n",
        "            morph = Morph(all_morph[0].split(\"\\t\")[0], all_morph[6], all_morph[0].split(\"\\t\")[1], all_morph[1])\n",
        "            sentence.append(morph)\n",
        "    return morph_data\n",
        "\n",
        "morph_result = generate_morph(\"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\")\n",
        "for m in morph_result[2]:\n",
        "    print(m.__dict__.values())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_values(['名前', '名前', '名詞', '一般'])\n",
            "dict_values(['は', 'は', '助詞', '係助詞'])\n",
            "dict_values(['まだ', 'まだ', '副詞', '助詞類接続'])\n",
            "dict_values(['無い', '無い', '形容詞', '自立'])\n",
            "dict_values(['。', '。', '記号', '句点'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HazrRkc7cSiy",
        "colab_type": "text"
      },
      "source": [
        "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
        "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNrQzzAfVn-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b5a07cc8-e767-4412-fd80-4b84be6aa609"
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "\n",
        "    戻り値：\n",
        "    1文のChunkクラスのリスト\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "        raise StopIteration\n",
        "\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for i, chunks in enumerate(generate_chunk(neko_file), 1):\n",
        "    # 8文目を表示\n",
        "    if i == 8:\n",
        "        for j, chunk in enumerate(chunks):\n",
        "            print('[{}]{}'.format(j, chunk))\n",
        "        break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]吾輩は\tsrcs[]\tdst[5]\n",
            "[1]ここで\tsrcs[]\tdst[2]\n",
            "[2]始めて\tsrcs[1]\tdst[3]\n",
            "[3]人間という\tsrcs[2]\tdst[4]\n",
            "[4]ものを\tsrcs[3]\tdst[5]\n",
            "[5]見た。\tsrcs[0, 4]\tdst[-1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOOUt-OTZ6t0",
        "colab_type": "text"
      },
      "source": [
        "## 42. 係り元と係り先の文節の表示\n",
        "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONayFJ5C4OoA",
        "colab_type": "code",
        "outputId": "dc4ed076-b51d-4524-e95c-1e46163f37f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def generate_related_cabocha(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        chunk_list = []\n",
        "        for c in chunk_data:\n",
        "            surface = \"\"\n",
        "            for morph in c.morphs:\n",
        "                surface += morph.surface\n",
        "            obj = {surface: {\"dst\": c.dst, \"srcs\": c.srcs}}\n",
        "            chunk_list.append(obj)\n",
        "        for clause in chunk_list:\n",
        "            srcs = clause[list(clause.keys())[0]][\"srcs\"]\n",
        "            if not len(srcs):\n",
        "                continue\n",
        "            for chunk_ind in srcs:\n",
        "                src_word = re.sub(r\"[。、]\", \"\", list(chunk_list[chunk_ind].keys())[0])\n",
        "                effected_word = re.sub(r\"[。、]\", \"\", list(clause.keys())[0])\n",
        "                result.append(\"{}\\t{}\".format(src_word, effected_word))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for cnt, word in enumerate(generate_related_cabocha(neko_file)):\n",
        "    # 量が多いので5行分だけ出力\n",
        "    if cnt > 5:\n",
        "        break\n",
        "    print(word)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "　\t猫である\n",
            "吾輩は\t猫である\n",
            "名前は\t無い\n",
            "まだ\t無い\n",
            "　どこで\t生れたか\n",
            "生れたか\tつかぬ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFVQR7mCtPC2",
        "colab_type": "text"
      },
      "source": [
        "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
        "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ． "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axjDXijiyGwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9473dbca-2bd9-4da6-b7c4-bb09239ba39a"
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def delete_panctuation(word):\n",
        "    return re.sub(r\"[、。]\", \"\", word)\n",
        "\n",
        "def generate_related_cabocha(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        chunk_list = []\n",
        "        for c in chunk_data:\n",
        "            surface = \"\"\n",
        "            pos_list = []\n",
        "            for morph in c.morphs:\n",
        "                surface += morph.surface\n",
        "                pos_list.append(morph.pos)\n",
        "            obj = {surface: {\"dst\": c.dst, \"srcs\": c.srcs, \"pos\": pos_list}}\n",
        "            chunk_list.append(obj)\n",
        "        for clause in chunk_list:\n",
        "            srcs = clause[list(clause.keys())[0]][\"srcs\"]\n",
        "            pos = clause[list(clause.keys())[0]][\"pos\"]\n",
        "            if not len(srcs):\n",
        "                continue\n",
        "            for chunk_ind in srcs:\n",
        "                src_word = list(chunk_list[chunk_ind].keys())[0]\n",
        "                effected_word = list(clause.keys())[0]\n",
        "                if \"名詞\" in chunk_list[chunk_ind][src_word][\"pos\"] and \"動詞\" in pos:\n",
        "                    # src:名詞 -> eff:動詞\n",
        "                    result.append(\"{}\\t{}\".format(\n",
        "                        delete_panctuation(src_word), delete_panctuation(effected_word)))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for cnt, word in enumerate(generate_related_cabocha(neko_file)):\n",
        "    # 量が多いので5行分だけ出力\n",
        "    if cnt > 5:\n",
        "        break\n",
        "    print(word)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "　どこで\t生れたか\n",
            "見当が\tつかぬ\n",
            "所で\t泣いて\n",
            "ニャーニャー\t泣いて\n",
            "いた事だけは\t記憶している\n",
            "ここで\t始めて\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--VFPoBdx7_S",
        "colab_type": "text"
      },
      "source": [
        "## 44. 係り受け木の可視化\n",
        "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Cf16tFzcfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install graphviz pydot pillow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLPO-DpNx7xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            if not len(self.srcs):\n",
        "                return\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def delete_panctuation(word):\n",
        "    return re.sub(r\"[、。]\", \"\", word)\n",
        "\n",
        "def generate_related_cabocha(neko_file):\n",
        "    result = []\n",
        "    for ind, chunk_data in enumerate(generate_chunk(neko_file)):\n",
        "        chunk_list = []\n",
        "        for c in chunk_data:\n",
        "            surface = \"\"\n",
        "            pos_list = []\n",
        "            for morph in c.morphs:\n",
        "                surface += morph.surface\n",
        "                pos_list.append(morph.pos)\n",
        "            obj = {surface: {\"dst\": c.dst, \"srcs\": c.srcs, \"pos\": pos_list}}\n",
        "            chunk_list.append(obj)\n",
        "        for clause in chunk_list:\n",
        "            srcs = clause[list(clause.keys())[0]][\"srcs\"]\n",
        "            pos = clause[list(clause.keys())[0]][\"pos\"]\n",
        "            if not len(srcs):\n",
        "                continue\n",
        "            for chunk_ind in srcs:\n",
        "                src_word = list(chunk_list[chunk_ind].keys())[0]\n",
        "                effected_word = list(clause.keys())[0]\n",
        "                if \"名詞\" in chunk_list[chunk_ind][src_word][\"pos\"] and \"動詞\" in pos:\n",
        "                    # src:名詞 -> eff:動詞\n",
        "                    result.append(\"{}\\t{}\".format(\n",
        "                        delete_panctuation(src_word), delete_panctuation(effected_word)))\n",
        "    return result\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for cnt, word in enumerate(generate_related_cabocha(neko_file)):\n",
        "    # 量が多いので5行分だけ出力\n",
        "    if cnt > 5:\n",
        "        break\n",
        "    print(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDiEkxGdzux3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydot import Dot, Node, Edge\n",
        "from PIL import Image\n",
        "from nlp41 import *\n",
        "\n",
        "\n",
        "def create_graph(sentence: [Chunk]) -> Dot:\n",
        "    graph = Dot(graph_type='graph')  # 有向グラフ\n",
        "    nodes = []\n",
        "\n",
        "    # まずノードを作っておく\n",
        "    for i, chunk in enumerate(sentence):\n",
        "        node = Node(f'\"{i}\"', label=str(chunk))\n",
        "        nodes.append(node)\n",
        "        graph.add_node(node)\n",
        "\n",
        "    # 次にエッジを登録\n",
        "    for i, chunk in enumerate(sentence):\n",
        "        if chunk.dst == -1:\n",
        "            continue\n",
        "        node_src = nodes[i]\n",
        "        node_dst = nodes[chunk.dst]\n",
        "        edge = Edge(node_src, node_dst)\n",
        "        graph.add_edge(edge)\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "sentences = generate_chunk(neko_file)\n",
        "\n",
        "sentence = sentences[5]\n",
        "\n",
        "# こっちの文を採用するとすごく長いよ\n",
        "# sentence = max(sentences, key=lambda s: len(s))\n",
        "\n",
        "graph = create_graph(sentence)\n",
        "\n",
        "# グラフを画像に書き出して表示\n",
        "graph.write_png('nlp44.png')\n",
        "Image.open('nlp44.png').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOeiHmYu6_tz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4f08f6e5-d806-41f9-d76f-e6b0859ff7d8"
      },
      "source": [
        "!wget http://beu.sakura.ne.jp/fontz/ARIALUNI.TTF\n",
        "!mv ARIALUNI.TTF /usr/local/share/fonts"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-16 07:14:30--  http://beu.sakura.ne.jp/fontz/ARIALUNI.TTF\n",
            "Resolving beu.sakura.ne.jp (beu.sakura.ne.jp)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘beu.sakura.ne.jp’\n",
            "mv: cannot stat 'ARIALUNI.TTF': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oog55LL1Hmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import CaboCha\n",
        "import codecs\n",
        "from pydot import Dot, Node, Edge\n",
        "from PIL import Image\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "class Morph(object):\n",
        "# 表層形 （Tab区切り）, 品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
        "    def __init__(self, surface, base, pos, pos1):\n",
        "        self.surface = surface\n",
        "        self.base = base\n",
        "        self.pos = pos\n",
        "        self.pos1 = pos1\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'.format(\n",
        "            self.surface, self.base, self.pos, self.pos1)\n",
        "\n",
        "class Chunk(object):\n",
        "# * 文節番号 係り先番号(-1) 主辞/機能語の位置 係り関係のスコア\n",
        "    def __init__(self):\n",
        "        self.morphs = []\n",
        "        self.dst = -1\n",
        "        self.srcs = []\n",
        "\n",
        "    def __str__(self):\n",
        "        surface = ''\n",
        "        for morph in self.morphs:\n",
        "            surface += morph.surface\n",
        "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
        "\n",
        "def generate_chunk(neko_file):\n",
        "    '''\n",
        "    係り受け解析結果を順次読み込んで、\n",
        "    1文ずつChunkクラスのリストを返す\n",
        "\n",
        "    戻り値：\n",
        "    1文のChunkクラスのリスト\n",
        "    '''\n",
        "    with open(neko_file) as file_parsed:\n",
        "        chunks = dict()     # idxをkeyにChunkを格納\n",
        "        idx = -1\n",
        "        for line in file_parsed:\n",
        "            if line == 'EOS\\n':\n",
        "                if len(chunks) > 0:\n",
        "                    # chunksをkeyでソートし、valueのみ取り出し\n",
        "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
        "                    yield list(zip(*sorted_tuple))[1]\n",
        "                    chunks.clear()\n",
        "                else:\n",
        "                    yield []\n",
        "            elif line[0] == '*':\n",
        "                cols = line.split(' ')\n",
        "                idx = int(cols[1])\n",
        "                dst = int(re.search(r'(.*?)D', cols[2]).group(1))\n",
        "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
        "                if idx not in chunks:\n",
        "                    chunks[idx] = Chunk()\n",
        "                chunks[idx].dst = dst\n",
        "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
        "                if dst != -1:\n",
        "                    if dst not in chunks:\n",
        "                        chunks[dst] = Chunk()\n",
        "                    chunks[dst].srcs.append(idx)\n",
        "            else:\n",
        "                cols = line.split('\\t')\n",
        "                res_cols = cols[1].split(',')\n",
        "                chunks[idx].morphs.append(\n",
        "                    Morph(\n",
        "                        cols[0],        # surface\n",
        "                        res_cols[6],    # base\n",
        "                        res_cols[0],    # pos\n",
        "                        res_cols[1]     # pos1\n",
        "                    )\n",
        "                )\n",
        "\n",
        "def create_graph(chunk_data):\n",
        "    graph = Dot(graph_type='graph')\n",
        "    nodes = []\n",
        "    for i, chunk in enumerate(chunk_data):\n",
        "        surface = \"\"\n",
        "        for w in chunk.morphs:\n",
        "            surface+=w.surface\n",
        "        node = Node(f'\"{i}\"', label=surface)\n",
        "        nodes.append(node)\n",
        "        graph.add_node(node)\n",
        "    for i, chunk in enumerate(chunk_data):\n",
        "        if chunk.dst == -1:\n",
        "            continue\n",
        "        node_src = nodes[i]\n",
        "        node_dst = nodes[chunk.dst]\n",
        "        edge = Edge(node_src, node_dst)\n",
        "        graph.add_edge(edge)\n",
        "    return graph\n",
        "\n",
        "\n",
        "neko_file = \"/content/drive/My Drive/Colab Notebooks/Python100本ノック/neko.txt.cabocha\"\n",
        "for i, chunks in enumerate(generate_chunk(neko_file), 1):\n",
        "    # 8文目を表示\n",
        "    if i == 8:\n",
        "        graph = create_graph(chunks)\n",
        "        graph.write_png('q44.png')\n",
        "        Image.open('q44.png').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFVk71Ne11Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}